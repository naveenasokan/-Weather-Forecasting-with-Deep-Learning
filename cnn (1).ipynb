{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"weather_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: torch.Size([977490, 3, 31])\n",
      "y_train shape: torch.Size([977490, 1])\n",
      "city_train shape: torch.Size([977490])\n",
      "X_test shape: torch.Size([108615, 3, 31])\n",
      "y_test shape: torch.Size([108615, 1])\n",
      "city_test shape: torch.Size([108615])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Convert boolean columns to integers (0 and 1)\n",
    "df = df.applymap(lambda x: int(x) if isinstance(x, bool) else x)\n",
    "\n",
    "# Encode the city names as integers\n",
    "df['city_idx'] = df['location_city'].astype('category').cat.codes  # Numeric index for each city\n",
    "features = ['humidity_2m', 'dew_point_2m', 'precip', 'snowfall', 'snow_depth', \n",
    "            'msl_pressure', 'surface_pressure', 'cloud_cover_low', 'cloud_cover_mid',\n",
    "            'cloud_cover_high', 'et0', 'vpd', 'wind_speed_10m', 'wind_dir_10m', \n",
    "            'soil_temp_0_7cm', 'soil_moisture_0_7cm', 'latitude', 'longitude'] + \\\n",
    "           [col for col in df.columns if col.startswith('weather_')]\n",
    "\n",
    "target = 'temp_2m'\n",
    "T = 3  # Number of timesteps\n",
    "input_dim = len(features)\n",
    "\n",
    "# Lists to hold training and testing data across all cities\n",
    "X_train_list, y_train_list, city_train_list = [], [], []\n",
    "X_test_list, y_test_list, city_test_list = [], [], []\n",
    "\n",
    "# To compute normalization statistics\n",
    "all_train_data = []\n",
    "\n",
    "# Process each city independently\n",
    "for city_id, city_data in df.groupby('city_idx'):\n",
    "    city_data = city_data.sort_values(by='time').reset_index(drop=True)\n",
    "    input_data = city_data[features].values\n",
    "    targets = city_data[target].values\n",
    "    \n",
    "    # Determine train size for the current city\n",
    "    N = len(input_data) - T  # Total number of sequences\n",
    "    train_size = int(0.90 * N)\n",
    "    \n",
    "    # Split data for normalization calculation (only on training set)\n",
    "    train_data = input_data[:train_size]\n",
    "    all_train_data.append(train_data)  # Collect all training data for global normalization\n",
    "    \n",
    "    # Convert to torch tensors\n",
    "    input_data_tensor = torch.tensor(input_data, dtype=torch.float32)\n",
    "    target_tensor = torch.tensor(targets, dtype=torch.float32)\n",
    "    \n",
    "    # Initialize train and test tensors for this city\n",
    "    X_train_city = torch.zeros((train_size, T, input_dim), dtype=torch.float32)\n",
    "    y_train_city = torch.zeros((train_size, 1), dtype=torch.float32)\n",
    "    X_test_city = torch.zeros((N - train_size, T, input_dim), dtype=torch.float32)\n",
    "    y_test_city = torch.zeros((N - train_size, 1), dtype=torch.float32)\n",
    "    \n",
    "    # Prepare training sequences\n",
    "    for t in range(train_size):\n",
    "        X_train_city[t] = input_data_tensor[t:t + T]\n",
    "        y_train_city[t] = target_tensor[t + T]\n",
    "    \n",
    "    # Prepare testing sequences\n",
    "    for i in range(N - train_size):\n",
    "        t = i + train_size\n",
    "        X_test_city[i] = input_data_tensor[t:t + T]\n",
    "        y_test_city[i] = target_tensor[t + T]\n",
    "    \n",
    "    # Prepare city indices for train and test\n",
    "    city_train_city = torch.full((train_size,), city_id, dtype=torch.long)\n",
    "    city_test_city = torch.full((N - train_size,), city_id, dtype=torch.long)\n",
    "    \n",
    "    # Append city-specific data to the main lists\n",
    "    X_train_list.append(X_train_city)\n",
    "    y_train_list.append(y_train_city)\n",
    "    city_train_list.append(city_train_city)\n",
    "    \n",
    "    X_test_list.append(X_test_city)\n",
    "    y_test_list.append(y_test_city)\n",
    "    city_test_list.append(city_test_city)\n",
    "\n",
    "# Concatenate all training data for normalization calculation\n",
    "all_train_data = np.vstack(all_train_data)\n",
    "train_mean = all_train_data.mean(axis=0)\n",
    "train_std = all_train_data.std(axis=0)\n",
    "\n",
    "# Concatenate all citiesâ€™ data to form the final tensors\n",
    "X_train = torch.cat(X_train_list, dim=0)\n",
    "y_train = torch.cat(y_train_list, dim=0)\n",
    "city_train = torch.cat(city_train_list, dim=0)\n",
    "\n",
    "X_test = torch.cat(X_test_list, dim=0)\n",
    "y_test = torch.cat(y_test_list, dim=0)\n",
    "city_test = torch.cat(city_test_list, dim=0)\n",
    "\n",
    "# Normalize X_train and X_test\n",
    "train_mean_tensor = torch.tensor(train_mean, dtype=torch.float32)\n",
    "train_std_tensor = torch.tensor(train_std, dtype=torch.float32)\n",
    "\n",
    "# Normalize along the feature dimension (last dimension)\n",
    "X_train = (X_train - train_mean_tensor) / train_std_tensor\n",
    "X_test = (X_test - train_mean_tensor) / train_std_tensor\n",
    "\n",
    "# Print shapes to confirm the results\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"city_train shape:\", city_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"city_test shape:\", city_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 22:50:47,179] A new study created in memory with name: no-name-358798cb-e4c3-4345-a8f4-93e7503feb9b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 22:52:04,473] Trial 0 finished with value: 508.5619718593085 and parameters: {'embedding_dim': 14, 'num_channels1': 112, 'num_channels2': 64, 'dropout_rate': 0.33380184650556455, 'learning_rate': 0.0004517897758154489, 'batch_size': 256}. Best is trial 0 with value: 508.5619718593085.\n",
      "[I 2024-11-30 22:52:09,717] Trial 2 finished with value: 524.1092224299067 and parameters: {'embedding_dim': 7, 'num_channels1': 128, 'num_channels2': 160, 'dropout_rate': 0.43173772319519815, 'learning_rate': 0.0004391997894515235, 'batch_size': 128}. Best is trial 0 with value: 508.5619718593085.\n",
      "[I 2024-11-30 22:52:21,621] Trial 1 finished with value: 479.9785383203177 and parameters: {'embedding_dim': 12, 'num_channels1': 96, 'num_channels2': 64, 'dropout_rate': 0.30258056667255423, 'learning_rate': 0.0009312602462542639, 'batch_size': 128}. Best is trial 1 with value: 479.9785383203177.\n",
      "[I 2024-11-30 22:52:59,435] Trial 3 finished with value: 463.97075144037717 and parameters: {'embedding_dim': 8, 'num_channels1': 32, 'num_channels2': 160, 'dropout_rate': 0.4212291875596265, 'learning_rate': 0.007912114104598768, 'batch_size': 64}. Best is trial 3 with value: 463.97075144037717.\n",
      "[I 2024-11-30 22:53:16,818] Trial 6 finished with value: 518.2584968477909 and parameters: {'embedding_dim': 15, 'num_channels1': 112, 'num_channels2': 240, 'dropout_rate': 0.45115116827496454, 'learning_rate': 0.00037512568453868797, 'batch_size': 256}. Best is trial 3 with value: 463.97075144037717.\n",
      "[I 2024-11-30 22:53:43,767] Trial 7 finished with value: 492.0381666262389 and parameters: {'embedding_dim': 15, 'num_channels1': 32, 'num_channels2': 112, 'dropout_rate': 0.4573523297782172, 'learning_rate': 0.003369691412078615, 'batch_size': 256}. Best is trial 3 with value: 463.97075144037717.\n",
      "[I 2024-11-30 22:54:28,117] Trial 9 finished with value: 480.0259320536708 and parameters: {'embedding_dim': 9, 'num_channels1': 112, 'num_channels2': 176, 'dropout_rate': 0.2619285015221004, 'learning_rate': 0.001672354559024046, 'batch_size': 256}. Best is trial 3 with value: 463.97075144037717.\n",
      "[I 2024-11-30 22:54:37,132] Trial 5 finished with value: 483.13294159619335 and parameters: {'embedding_dim': 15, 'num_channels1': 96, 'num_channels2': 96, 'dropout_rate': 0.4073505826556173, 'learning_rate': 0.0004421528767265122, 'batch_size': 64}. Best is trial 3 with value: 463.97075144037717.\n",
      "[I 2024-11-30 22:54:43,705] Trial 8 finished with value: 473.38004413246455 and parameters: {'embedding_dim': 12, 'num_channels1': 32, 'num_channels2': 144, 'dropout_rate': 0.329727252655593, 'learning_rate': 0.0019393506003166552, 'batch_size': 128}. Best is trial 3 with value: 463.97075144037717.\n",
      "[I 2024-11-30 22:55:14,197] Trial 10 finished with value: 461.0754470182621 and parameters: {'embedding_dim': 6, 'num_channels1': 80, 'num_channels2': 112, 'dropout_rate': 0.357029886960192, 'learning_rate': 0.003919723791445368, 'batch_size': 256}. Best is trial 10 with value: 461.0754470182621.\n",
      "[I 2024-11-30 22:55:32,897] Trial 4 finished with value: 444.12348398151244 and parameters: {'embedding_dim': 11, 'num_channels1': 80, 'num_channels2': 80, 'dropout_rate': 0.42014608539100445, 'learning_rate': 0.0010414933968848517, 'batch_size': 64}. Best is trial 4 with value: 444.12348398151244.\n",
      "[I 2024-11-30 22:55:59,699] Trial 13 finished with value: 506.1655051367323 and parameters: {'embedding_dim': 4, 'num_channels1': 64, 'num_channels2': 208, 'dropout_rate': 0.20350252224095244, 'learning_rate': 0.009552024981085722, 'batch_size': 256}. Best is trial 4 with value: 444.12348398151244.\n",
      "[I 2024-11-30 22:56:04,033] Trial 11 finished with value: 473.9440035924566 and parameters: {'embedding_dim': 15, 'num_channels1': 112, 'num_channels2': 112, 'dropout_rate': 0.3442760657633704, 'learning_rate': 0.001923430076145661, 'batch_size': 128}. Best is trial 4 with value: 444.12348398151244.\n",
      "[I 2024-11-30 22:56:13,935] Trial 12 finished with value: 503.8812992320285 and parameters: {'embedding_dim': 8, 'num_channels1': 112, 'num_channels2': 64, 'dropout_rate': 0.45540335402757653, 'learning_rate': 0.009075369151471637, 'batch_size': 128}. Best is trial 4 with value: 444.12348398151244.\n",
      "[I 2024-11-30 22:57:44,476] Trial 15 finished with value: 446.4692274631526 and parameters: {'embedding_dim': 5, 'num_channels1': 64, 'num_channels2': 112, 'dropout_rate': 0.4992656903083967, 'learning_rate': 0.004013735150482463, 'batch_size': 64}. Best is trial 4 with value: 444.12348398151244.\n",
      "[I 2024-11-30 22:57:54,924] Trial 16 finished with value: 508.0131663216251 and parameters: {'embedding_dim': 5, 'num_channels1': 64, 'num_channels2': 96, 'dropout_rate': 0.3868783993253946, 'learning_rate': 0.00013930184450220012, 'batch_size': 64}. Best is trial 4 with value: 444.12348398151244.\n",
      "[I 2024-11-30 22:58:40,517] Trial 17 finished with value: 507.15869136868844 and parameters: {'embedding_dim': 5, 'num_channels1': 64, 'num_channels2': 96, 'dropout_rate': 0.4968669760322818, 'learning_rate': 0.0001091494354449984, 'batch_size': 64}. Best is trial 4 with value: 444.12348398151244.\n",
      "[I 2024-11-30 22:59:20,442] Trial 14 finished with value: 485.79101397528973 and parameters: {'embedding_dim': 4, 'num_channels1': 64, 'num_channels2': 112, 'dropout_rate': 0.3751037666347089, 'learning_rate': 0.00013259785686959918, 'batch_size': 64}. Best is trial 4 with value: 444.12348398151244.\n",
      "[I 2024-11-30 22:59:28,116] Trial 18 finished with value: 541.3285820925287 and parameters: {'embedding_dim': 11, 'num_channels1': 64, 'num_channels2': 128, 'dropout_rate': 0.4893238158005989, 'learning_rate': 0.00010251231742501823, 'batch_size': 64}. Best is trial 4 with value: 444.12348398151244.\n",
      "[I 2024-11-30 22:59:46,802] Trial 19 finished with value: 521.4192990423288 and parameters: {'embedding_dim': 12, 'num_channels1': 64, 'num_channels2': 128, 'dropout_rate': 0.49746890312282593, 'learning_rate': 0.0009054787719073553, 'batch_size': 64}. Best is trial 4 with value: 444.12348398151244.\n",
      "[I 2024-11-30 23:01:02,484] Trial 20 finished with value: 499.22344516386494 and parameters: {'embedding_dim': 12, 'num_channels1': 48, 'num_channels2': 128, 'dropout_rate': 0.4932066634699598, 'learning_rate': 0.0008095723445170198, 'batch_size': 64}. Best is trial 4 with value: 444.12348398151244.\n",
      "[I 2024-11-30 23:01:15,990] Trial 22 finished with value: 500.35046321768203 and parameters: {'embedding_dim': 10, 'num_channels1': 48, 'num_channels2': 192, 'dropout_rate': 0.4686549025732402, 'learning_rate': 0.0009776926828814244, 'batch_size': 64}. Best is trial 4 with value: 444.12348398151244.\n",
      "[I 2024-11-30 23:01:52,896] Trial 23 finished with value: 442.7476459150507 and parameters: {'embedding_dim': 10, 'num_channels1': 48, 'num_channels2': 192, 'dropout_rate': 0.4642515047432368, 'learning_rate': 0.003800364581982227, 'batch_size': 64}. Best is trial 23 with value: 442.7476459150507.\n",
      "[I 2024-11-30 23:01:55,949] Trial 24 finished with value: 477.30916692088624 and parameters: {'embedding_dim': 6, 'num_channels1': 80, 'num_channels2': 80, 'dropout_rate': 0.3816486490627597, 'learning_rate': 0.003986289434578161, 'batch_size': 256}. Best is trial 23 with value: 442.7476459150507.\n",
      "[I 2024-11-30 23:02:03,992] Trial 25 finished with value: 492.448657244547 and parameters: {'embedding_dim': 6, 'num_channels1': 80, 'num_channels2': 80, 'dropout_rate': 0.3745819871450388, 'learning_rate': 0.004639256554667349, 'batch_size': 256}. Best is trial 23 with value: 442.7476459150507.\n",
      "[I 2024-11-30 23:02:57,864] Trial 21 finished with value: 436.6103433652799 and parameters: {'embedding_dim': 11, 'num_channels1': 48, 'num_channels2': 144, 'dropout_rate': 0.48133959256849546, 'learning_rate': 0.000929916407293383, 'batch_size': 64}. Best is trial 21 with value: 436.6103433652799.\n",
      "[I 2024-11-30 23:03:47,127] Trial 26 finished with value: 496.32917810298386 and parameters: {'embedding_dim': 10, 'num_channels1': 80, 'num_channels2': 224, 'dropout_rate': 0.43596839303683094, 'learning_rate': 0.004734077777134863, 'batch_size': 64}. Best is trial 21 with value: 436.6103433652799.\n",
      "[I 2024-11-30 23:03:48,726] Trial 28 finished with value: 479.1654600368473 and parameters: {'embedding_dim': 10, 'num_channels1': 48, 'num_channels2': 224, 'dropout_rate': 0.43504118258726004, 'learning_rate': 0.006391397330953401, 'batch_size': 64}. Best is trial 21 with value: 436.6103433652799.\n",
      "[I 2024-11-30 23:04:19,450] Trial 27 finished with value: 431.5030047937139 and parameters: {'embedding_dim': 10, 'num_channels1': 48, 'num_channels2': 240, 'dropout_rate': 0.4324172994616102, 'learning_rate': 0.005709437032900578, 'batch_size': 64}. Best is trial 27 with value: 431.5030047937139.\n",
      "[I 2024-11-30 23:05:32,974] Trial 31 finished with value: 482.6631519642192 and parameters: {'embedding_dim': 13, 'num_channels1': 48, 'num_channels2': 256, 'dropout_rate': 0.40599825049040006, 'learning_rate': 0.002270191385233664, 'batch_size': 64}. Best is trial 27 with value: 431.5030047937139.\n",
      "[I 2024-11-30 23:05:37,156] Trial 30 finished with value: 511.37684626795306 and parameters: {'embedding_dim': 13, 'num_channels1': 48, 'num_channels2': 256, 'dropout_rate': 0.4130714397872408, 'learning_rate': 0.0013250480910835215, 'batch_size': 64}. Best is trial 27 with value: 431.5030047937139.\n",
      "[I 2024-11-30 23:06:35,266] Trial 29 finished with value: 543.8773449559033 and parameters: {'embedding_dim': 10, 'num_channels1': 48, 'num_channels2': 224, 'dropout_rate': 0.4293068900883636, 'learning_rate': 0.0015202123752645945, 'batch_size': 64}. Best is trial 27 with value: 431.5030047937139.\n",
      "[I 2024-11-30 23:06:39,801] Trial 32 finished with value: 483.6805609606701 and parameters: {'embedding_dim': 13, 'num_channels1': 48, 'num_channels2': 256, 'dropout_rate': 0.4689845258310894, 'learning_rate': 0.0030087093187099664, 'batch_size': 64}. Best is trial 27 with value: 431.5030047937139.\n",
      "[I 2024-11-30 23:07:20,368] Trial 33 finished with value: 509.72372934771136 and parameters: {'embedding_dim': 9, 'num_channels1': 48, 'num_channels2': 192, 'dropout_rate': 0.47127652414005916, 'learning_rate': 0.0002309313039237785, 'batch_size': 64}. Best is trial 27 with value: 431.5030047937139.\n",
      "[I 2024-11-30 23:07:30,624] Trial 34 finished with value: 527.9903527107028 and parameters: {'embedding_dim': 9, 'num_channels1': 32, 'num_channels2': 192, 'dropout_rate': 0.46809827500699536, 'learning_rate': 0.002831575144370402, 'batch_size': 64}. Best is trial 27 with value: 431.5030047937139.\n",
      "[I 2024-11-30 23:09:04,938] Trial 36 finished with value: 577.615010430867 and parameters: {'embedding_dim': 11, 'num_channels1': 96, 'num_channels2': 192, 'dropout_rate': 0.4727396238200956, 'learning_rate': 0.0006076409338114199, 'batch_size': 64}. Best is trial 27 with value: 431.5030047937139.\n",
      "[I 2024-11-30 23:09:07,565] Trial 37 finished with value: 459.2944873854308 and parameters: {'embedding_dim': 11, 'num_channels1': 96, 'num_channels2': 160, 'dropout_rate': 0.44819069735777467, 'learning_rate': 0.0005319176416497566, 'batch_size': 64}. Best is trial 27 with value: 431.5030047937139.\n",
      "[I 2024-11-30 23:09:26,835] Trial 38 finished with value: 517.0183817523092 and parameters: {'embedding_dim': 11, 'num_channels1': 96, 'num_channels2': 176, 'dropout_rate': 0.400207848370293, 'learning_rate': 0.006090692527840121, 'batch_size': 64}. Best is trial 27 with value: 431.5030047937139.\n",
      "[I 2024-11-30 23:10:15,829] Trial 40 finished with value: 453.5235585618649 and parameters: {'embedding_dim': 8, 'num_channels1': 32, 'num_channels2': 176, 'dropout_rate': 0.4013086293001041, 'learning_rate': 0.0068899277409329085, 'batch_size': 128}. Best is trial 27 with value: 431.5030047937139.\n",
      "[I 2024-11-30 23:10:18,859] Trial 35 finished with value: 507.88906042259407 and parameters: {'embedding_dim': 11, 'num_channels1': 96, 'num_channels2': 192, 'dropout_rate': 0.47595843006788563, 'learning_rate': 0.0006647608480754539, 'batch_size': 64}. Best is trial 27 with value: 431.5030047937139.\n",
      "[I 2024-11-30 23:10:33,355] Trial 39 finished with value: 511.92368716554313 and parameters: {'embedding_dim': 11, 'num_channels1': 128, 'num_channels2': 160, 'dropout_rate': 0.40044272793733576, 'learning_rate': 0.005969167901872812, 'batch_size': 128}. Best is trial 27 with value: 431.5030047937139.\n",
      "[I 2024-11-30 23:10:41,442] Trial 41 finished with value: 468.1946345374831 and parameters: {'embedding_dim': 8, 'num_channels1': 32, 'num_channels2': 144, 'dropout_rate': 0.31939074881828056, 'learning_rate': 0.0012026213602877437, 'batch_size': 128}. Best is trial 27 with value: 431.5030047937139.\n",
      "[I 2024-11-30 23:12:08,369] Trial 42 finished with value: 515.3671292864552 and parameters: {'embedding_dim': 14, 'num_channels1': 32, 'num_channels2': 144, 'dropout_rate': 0.4397204133076472, 'learning_rate': 0.00032647754596421615, 'batch_size': 64}. Best is trial 27 with value: 431.5030047937139.\n",
      "[I 2024-11-30 23:12:21,823] Trial 43 finished with value: 506.02195963279394 and parameters: {'embedding_dim': 8, 'num_channels1': 128, 'num_channels2': 144, 'dropout_rate': 0.4215978446149564, 'learning_rate': 0.00034874801704102963, 'batch_size': 128}. Best is trial 27 with value: 431.5030047937139.\n",
      "[I 2024-11-30 23:12:36,604] Trial 45 finished with value: 466.37808969009427 and parameters: {'embedding_dim': 14, 'num_channels1': 64, 'num_channels2': 144, 'dropout_rate': 0.4414462826753515, 'learning_rate': 0.0003311130986925544, 'batch_size': 64}. Best is trial 27 with value: 431.5030047937139.\n",
      "[I 2024-11-30 23:13:08,723] Trial 44 finished with value: 502.3715768708006 and parameters: {'embedding_dim': 14, 'num_channels1': 64, 'num_channels2': 144, 'dropout_rate': 0.4507683758990963, 'learning_rate': 0.0024138846593691325, 'batch_size': 64}. Best is trial 27 with value: 431.5030047937139.\n",
      "[I 2024-11-30 23:13:52,603] Trial 46 finished with value: 530.9440505754765 and parameters: {'embedding_dim': 16, 'num_channels1': 80, 'num_channels2': 64, 'dropout_rate': 0.48306596828928394, 'learning_rate': 0.0024143237931059476, 'batch_size': 64}. Best is trial 27 with value: 431.5030047937139.\n",
      "[I 2024-11-30 23:14:25,460] Trial 48 finished with value: 490.2465030313753 and parameters: {'embedding_dim': 16, 'num_channels1': 64, 'num_channels2': 64, 'dropout_rate': 0.2791246210505259, 'learning_rate': 0.0025294906676494385, 'batch_size': 64}. Best is trial 27 with value: 431.5030047937139.\n",
      "[I 2024-11-30 23:15:32,737] Trial 49 finished with value: 584.0192747208118 and parameters: {'embedding_dim': 9, 'num_channels1': 80, 'num_channels2': 64, 'dropout_rate': 0.48608652952493153, 'learning_rate': 0.0037573520987555895, 'batch_size': 64}. Best is trial 27 with value: 431.5030047937139.\n",
      "[I 2024-11-30 23:15:33,683] Trial 50 finished with value: 408.8404535665214 and parameters: {'embedding_dim': 9, 'num_channels1': 48, 'num_channels2': 80, 'dropout_rate': 0.260359973377371, 'learning_rate': 0.007897233987265898, 'batch_size': 64}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:16:00,494] Trial 47 finished with value: 463.2032600707259 and parameters: {'embedding_dim': 9, 'num_channels1': 80, 'num_channels2': 80, 'dropout_rate': 0.4841560362684927, 'learning_rate': 0.0021520486620499185, 'batch_size': 64}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:16:14,631] Trial 53 finished with value: 434.51567713316564 and parameters: {'embedding_dim': 7, 'num_channels1': 32, 'num_channels2': 80, 'dropout_rate': 0.236287702636683, 'learning_rate': 0.007747683876577204, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:16:29,342] Trial 51 finished with value: 490.4782087903289 and parameters: {'embedding_dim': 9, 'num_channels1': 80, 'num_channels2': 240, 'dropout_rate': 0.4848159620670985, 'learning_rate': 0.00356507146133004, 'batch_size': 64}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:17:00,457] Trial 55 finished with value: 414.8754391144154 and parameters: {'embedding_dim': 7, 'num_channels1': 32, 'num_channels2': 96, 'dropout_rate': 0.21577615297212369, 'learning_rate': 0.008303651123622515, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:17:20,461] Trial 56 finished with value: 423.2689062979589 and parameters: {'embedding_dim': 7, 'num_channels1': 32, 'num_channels2': 80, 'dropout_rate': 0.2266123061894227, 'learning_rate': 0.008078399008513974, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:17:49,387] Trial 57 finished with value: 445.1967109237118 and parameters: {'embedding_dim': 7, 'num_channels1': 32, 'num_channels2': 96, 'dropout_rate': 0.2196984593448593, 'learning_rate': 0.008042309028155979, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:18:10,395] Trial 58 finished with value: 510.95760735949096 and parameters: {'embedding_dim': 7, 'num_channels1': 32, 'num_channels2': 96, 'dropout_rate': 0.21693533658889377, 'learning_rate': 0.008239738536465379, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:18:27,718] Trial 52 finished with value: 476.3707923814225 and parameters: {'embedding_dim': 7, 'num_channels1': 48, 'num_channels2': 240, 'dropout_rate': 0.3585827640686222, 'learning_rate': 0.004952231197638216, 'batch_size': 64}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:18:39,485] Trial 59 finished with value: 462.18642666830175 and parameters: {'embedding_dim': 7, 'num_channels1': 32, 'num_channels2': 80, 'dropout_rate': 0.24091310729713564, 'learning_rate': 0.009612187279850758, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:19:06,904] Trial 60 finished with value: 450.41184621100194 and parameters: {'embedding_dim': 7, 'num_channels1': 32, 'num_channels2': 80, 'dropout_rate': 0.2461142869968648, 'learning_rate': 0.004742009478785803, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:19:25,860] Trial 54 finished with value: 564.9319533155241 and parameters: {'embedding_dim': 7, 'num_channels1': 48, 'num_channels2': 96, 'dropout_rate': 0.22208118600537224, 'learning_rate': 0.008370046630317585, 'batch_size': 64}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:19:34,628] Trial 61 finished with value: 438.08332860301505 and parameters: {'embedding_dim': 6, 'num_channels1': 32, 'num_channels2': 80, 'dropout_rate': 0.23474685245553592, 'learning_rate': 0.005193293320155361, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:19:42,075] Trial 62 finished with value: 466.95808595866936 and parameters: {'embedding_dim': 6, 'num_channels1': 32, 'num_channels2': 80, 'dropout_rate': 0.2524199256170847, 'learning_rate': 0.005480194654809435, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:20:16,646] Trial 63 finished with value: 468.20882116909036 and parameters: {'embedding_dim': 6, 'num_channels1': 32, 'num_channels2': 96, 'dropout_rate': 0.2827242628806189, 'learning_rate': 0.007446565253683686, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:20:48,178] Trial 64 finished with value: 411.13115883074926 and parameters: {'embedding_dim': 6, 'num_channels1': 32, 'num_channels2': 208, 'dropout_rate': 0.27773515860376574, 'learning_rate': 0.007117909256807906, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:20:49,063] Trial 65 finished with value: 487.2406505806245 and parameters: {'embedding_dim': 6, 'num_channels1': 32, 'num_channels2': 112, 'dropout_rate': 0.2661531010035889, 'learning_rate': 0.00701843209378032, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:20:49,600] Trial 66 finished with value: 503.3910889638546 and parameters: {'embedding_dim': 6, 'num_channels1': 32, 'num_channels2': 112, 'dropout_rate': 0.2777848480299516, 'learning_rate': 0.00740541426614127, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:21:25,708] Trial 67 finished with value: 577.1832232701611 and parameters: {'embedding_dim': 6, 'num_channels1': 32, 'num_channels2': 112, 'dropout_rate': 0.20175611385987993, 'learning_rate': 0.009922531233144863, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:21:57,669] Trial 70 finished with value: 456.1222660438908 and parameters: {'embedding_dim': 5, 'num_channels1': 48, 'num_channels2': 208, 'dropout_rate': 0.20373367256821065, 'learning_rate': 0.009080236933569892, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:22:04,252] Trial 69 finished with value: 433.2057798110465 and parameters: {'embedding_dim': 8, 'num_channels1': 48, 'num_channels2': 208, 'dropout_rate': 0.22900262597766044, 'learning_rate': 0.009339975073101537, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:22:10,263] Trial 68 finished with value: 481.88250023941964 and parameters: {'embedding_dim': 8, 'num_channels1': 32, 'num_channels2': 112, 'dropout_rate': 0.27480793724203284, 'learning_rate': 0.0071980964464045385, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:22:36,839] Trial 71 finished with value: 532.067269235029 and parameters: {'embedding_dim': 5, 'num_channels1': 48, 'num_channels2': 208, 'dropout_rate': 0.23119814626472995, 'learning_rate': 0.0059219690022960705, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:23:06,427] Trial 72 finished with value: 519.5238631542713 and parameters: {'embedding_dim': 5, 'num_channels1': 48, 'num_channels2': 64, 'dropout_rate': 0.22998630216960173, 'learning_rate': 0.006059286432964449, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:23:21,566] Trial 73 finished with value: 522.9472624072596 and parameters: {'embedding_dim': 8, 'num_channels1': 48, 'num_channels2': 208, 'dropout_rate': 0.2288233498093528, 'learning_rate': 0.006392733370780756, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:23:34,111] Trial 74 finished with value: 586.8301222153133 and parameters: {'embedding_dim': 8, 'num_channels1': 48, 'num_channels2': 208, 'dropout_rate': 0.2326283987517323, 'learning_rate': 0.0061250689157280325, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:23:46,575] Trial 75 finished with value: 536.3457939223822 and parameters: {'embedding_dim': 8, 'num_channels1': 48, 'num_channels2': 224, 'dropout_rate': 0.29171238051464043, 'learning_rate': 0.008390024723148974, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:24:14,538] Trial 76 finished with value: 492.6637499898837 and parameters: {'embedding_dim': 8, 'num_channels1': 48, 'num_channels2': 224, 'dropout_rate': 0.29830037612931803, 'learning_rate': 0.008940442847740192, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:24:37,092] Trial 77 finished with value: 485.85123965868115 and parameters: {'embedding_dim': 10, 'num_channels1': 48, 'num_channels2': 224, 'dropout_rate': 0.25157196166671886, 'learning_rate': 0.0007683521840091191, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:24:55,400] Trial 78 finished with value: 508.062143189001 and parameters: {'embedding_dim': 9, 'num_channels1': 48, 'num_channels2': 224, 'dropout_rate': 0.2518837827342137, 'learning_rate': 0.0043213055840778765, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:24:57,362] Trial 79 finished with value: 511.5411601262007 and parameters: {'embedding_dim': 10, 'num_channels1': 32, 'num_channels2': 240, 'dropout_rate': 0.2523422350430215, 'learning_rate': 0.004337738576032839, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:25:22,687] Trial 80 finished with value: 436.46419892125516 and parameters: {'embedding_dim': 7, 'num_channels1': 32, 'num_channels2': 128, 'dropout_rate': 0.26050059525976893, 'learning_rate': 0.004548767078609444, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:25:52,855] Trial 81 finished with value: 485.2974555927087 and parameters: {'embedding_dim': 10, 'num_channels1': 32, 'num_channels2': 240, 'dropout_rate': 0.2114479064029389, 'learning_rate': 0.0011765217145698291, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:26:08,419] Trial 83 finished with value: 504.79226787269744 and parameters: {'embedding_dim': 7, 'num_channels1': 64, 'num_channels2': 128, 'dropout_rate': 0.2631272978591512, 'learning_rate': 0.00689918615500216, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:26:19,214] Trial 82 finished with value: 459.4277840038133 and parameters: {'embedding_dim': 12, 'num_channels1': 32, 'num_channels2': 128, 'dropout_rate': 0.31746663746490106, 'learning_rate': 0.009800411155773464, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:26:32,451] Trial 84 finished with value: 487.10140492494884 and parameters: {'embedding_dim': 7, 'num_channels1': 32, 'num_channels2': 128, 'dropout_rate': 0.20951665564720512, 'learning_rate': 0.0011038081744254166, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:27:06,268] Trial 85 finished with value: 479.2293842978387 and parameters: {'embedding_dim': 7, 'num_channels1': 32, 'num_channels2': 128, 'dropout_rate': 0.2630722500213417, 'learning_rate': 0.009987003062298655, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:27:13,894] Trial 86 finished with value: 487.5164326697175 and parameters: {'embedding_dim': 7, 'num_channels1': 32, 'num_channels2': 128, 'dropout_rate': 0.240509122768103, 'learning_rate': 0.0055716004844261644, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:27:39,857] Trial 87 finished with value: 455.26393838862543 and parameters: {'embedding_dim': 7, 'num_channels1': 32, 'num_channels2': 176, 'dropout_rate': 0.2101913032922211, 'learning_rate': 0.005386261437164351, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:27:59,239] Trial 88 finished with value: 545.6596902422732 and parameters: {'embedding_dim': 4, 'num_channels1': 32, 'num_channels2': 176, 'dropout_rate': 0.24144777753131064, 'learning_rate': 0.0052998222153289665, 'batch_size': 128}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:28:35,356] Trial 90 finished with value: 526.5280576831685 and parameters: {'embedding_dim': 4, 'num_channels1': 48, 'num_channels2': 176, 'dropout_rate': 0.2714722807601554, 'learning_rate': 0.007959457612909546, 'batch_size': 128}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:28:45,903] Trial 89 finished with value: 491.727997774671 and parameters: {'embedding_dim': 9, 'num_channels1': 32, 'num_channels2': 176, 'dropout_rate': 0.24533007836413417, 'learning_rate': 0.005454443038461063, 'batch_size': 128}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:29:26,219] Trial 92 finished with value: 505.22853784728596 and parameters: {'embedding_dim': 9, 'num_channels1': 48, 'num_channels2': 160, 'dropout_rate': 0.2244806354950715, 'learning_rate': 0.003171437705740561, 'batch_size': 128}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:29:36,242] Trial 93 finished with value: 480.8628600963689 and parameters: {'embedding_dim': 9, 'num_channels1': 48, 'num_channels2': 160, 'dropout_rate': 0.2572210885444189, 'learning_rate': 0.003172970736899131, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:29:50,120] Trial 91 finished with value: 423.35156724867755 and parameters: {'embedding_dim': 9, 'num_channels1': 48, 'num_channels2': 96, 'dropout_rate': 0.2713427637880226, 'learning_rate': 0.0032529421417707392, 'batch_size': 128}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:29:54,921] Trial 94 finished with value: 466.84003015095493 and parameters: {'embedding_dim': 6, 'num_channels1': 32, 'num_channels2': 80, 'dropout_rate': 0.22601172709047923, 'learning_rate': 0.0032286428027296626, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:30:25,288] Trial 95 finished with value: 518.4638199139168 and parameters: {'embedding_dim': 6, 'num_channels1': 32, 'num_channels2': 80, 'dropout_rate': 0.2348430394127616, 'learning_rate': 0.006638885163988293, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:30:35,970] Trial 96 finished with value: 478.0798372511983 and parameters: {'embedding_dim': 6, 'num_channels1': 32, 'num_channels2': 80, 'dropout_rate': 0.23436733508870394, 'learning_rate': 0.0015509470710669655, 'batch_size': 256}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:31:14,678] Trial 98 finished with value: 473.8467930246146 and parameters: {'embedding_dim': 11, 'num_channels1': 48, 'num_channels2': 96, 'dropout_rate': 0.2147389371817628, 'learning_rate': 0.006771852910475636, 'batch_size': 128}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:31:20,436] Trial 97 finished with value: 419.83632606970014 and parameters: {'embedding_dim': 8, 'num_channels1': 64, 'num_channels2': 96, 'dropout_rate': 0.2865259916908619, 'learning_rate': 0.0066143241456709835, 'batch_size': 128}. Best is trial 50 with value: 408.8404535665214.\n",
      "[I 2024-11-30 23:31:22,616] Trial 99 finished with value: 444.07689991706866 and parameters: {'embedding_dim': 8, 'num_channels1': 48, 'num_channels2': 96, 'dropout_rate': 0.2890149411521855, 'learning_rate': 0.0014980745062995612, 'batch_size': 128}. Best is trial 50 with value: 408.8404535665214.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'embedding_dim': 9, 'num_channels1': 48, 'num_channels2': 80, 'dropout_rate': 0.260359973377371, 'learning_rate': 0.007897233987265898, 'batch_size': 64}\n",
      "Best Validation Loss: 408.8404535665214\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import numpy as np\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "class CNNModelWithEmbedding(nn.Module):\n",
    "    def __init__(self, input_dim, num_channels1, num_channels2, embedding_dim, num_cities, dropout_rate):\n",
    "        super(CNNModelWithEmbedding, self).__init__()\n",
    "        \n",
    "        # Embedding for city index\n",
    "        self.city_embedding = nn.Embedding(num_embeddings=num_cities, embedding_dim=embedding_dim)\n",
    "        \n",
    "        # CNN layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim + embedding_dim, out_channels=num_channels1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=num_channels1, out_channels=num_channels2, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(num_channels2 * (T // 2), 64)  # Adjust based on sequence length (T)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        self.output = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, x, city_idx):\n",
    "        # Add city embeddings\n",
    "        city_emb = self.city_embedding(city_idx).unsqueeze(1).expand(-1, x.size(1), -1)\n",
    "        x = torch.cat([x, city_emb], dim=2)\n",
    "        \n",
    "        # Permute for CNN (channels-first format)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # Pass through CNN layers\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        \n",
    "        # Flatten for fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "        x = self.dropout(self.fc1(x))\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(self.fc2(x))\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    # Sample hyperparameters\n",
    "    embedding_dim = trial.suggest_int(\"embedding_dim\", 4, 16)\n",
    "    num_channels1 = trial.suggest_int(\"num_channels1\", 32, 128, step=16)\n",
    "    num_channels2 = trial.suggest_int(\"num_channels2\", 64, 256, step=16)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.2, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
    "\n",
    "    n_splits_per_city = 5\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits_per_city)\n",
    "\n",
    "    # Store city-specific splits\n",
    "    city_splits = []\n",
    "\n",
    "    for city in city_train.unique().tolist():  # Get unique city IDs\n",
    "        city_mask = (city_train == city)\n",
    "        city_X = X_train[city_mask]\n",
    "        city_y = y_train[city_mask]\n",
    "        \n",
    "        # Subsample 50% of the data for faster tuning\n",
    "        subsample_size = int(0.1 * len(city_X))\n",
    "        subsample_indices = torch.arange(len(city_X))[:subsample_size]\n",
    "        city_X = city_X[subsample_indices]\n",
    "        city_y = city_y[subsample_indices]\n",
    "\n",
    "        # Split city data temporally\n",
    "        for train_idx, val_idx in tscv.split(city_X):\n",
    "            city_splits.append((\n",
    "                city_X[train_idx], city_X[val_idx],\n",
    "                city_y[train_idx], city_y[val_idx],\n",
    "                torch.full((len(train_idx),), city, dtype=torch.long),\n",
    "                torch.full((len(val_idx),), city, dtype=torch.long)\n",
    "            ))\n",
    "\n",
    "    # Combine city-specific splits into global folds\n",
    "    n_global_folds = 5\n",
    "    global_folds = [[] for _ in range(n_global_folds)]\n",
    "\n",
    "    for i, split in enumerate(city_splits):\n",
    "        global_folds[i % n_global_folds].append(split)\n",
    "\n",
    "    # Combine splits within each global fold\n",
    "    combined_folds = []\n",
    "    for fold in global_folds:\n",
    "        X_train_fold = torch.cat([f[0] for f in fold])\n",
    "        X_val_fold = torch.cat([f[1] for f in fold])\n",
    "        y_train_fold = torch.cat([f[2] for f in fold])\n",
    "        y_val_fold = torch.cat([f[3] for f in fold])\n",
    "        city_train_fold = torch.cat([f[4] for f in fold])\n",
    "        city_val_fold = torch.cat([f[5] for f in fold])\n",
    "\n",
    "        combined_folds.append((X_train_fold, X_val_fold, y_train_fold, y_val_fold, city_train_fold, city_val_fold))\n",
    "\n",
    "    # Perform training and validation\n",
    "    fold_val_losses = []\n",
    "\n",
    "    for X_train_fold, X_val_fold, y_train_fold, y_val_fold, city_train_fold, city_val_fold in combined_folds:\n",
    "        train_dataset = torch.utils.data.TensorDataset(X_train_fold, y_train_fold, city_train_fold)\n",
    "        val_dataset = torch.utils.data.TensorDataset(X_val_fold, y_val_fold, city_val_fold)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Initialize model\n",
    "        model = CNNModelWithEmbedding(\n",
    "            input_dim=X_train.shape[2],\n",
    "            num_channels1=num_channels1,\n",
    "            num_channels2=num_channels2,\n",
    "            embedding_dim=embedding_dim,\n",
    "            num_cities=city_train.max().item() + 1,\n",
    "            dropout_rate=dropout_rate\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        loss_fn = nn.MSELoss()\n",
    "\n",
    "        # Train\n",
    "        for epoch in range(5):  # Use fewer epochs for tuning\n",
    "            model.train()\n",
    "            for X_batch, y_batch, city_batch in train_loader:\n",
    "                X_batch, y_batch, city_batch = X_batch.to(device), y_batch.to(device), city_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(X_batch, city_batch).squeeze()\n",
    "                loss = loss_fn(output, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch, city_batch in val_loader:\n",
    "                X_batch, y_batch, city_batch = X_batch.to(device), y_batch.to(device), city_batch.to(device)\n",
    "                output = model(X_batch, city_batch).squeeze()\n",
    "                loss = loss_fn(output, y_batch)\n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        fold_val_losses.append(val_loss)\n",
    "\n",
    "    avg_val_loss = np.mean(fold_val_losses)\n",
    "    return avg_val_loss\n",
    "\n",
    "\n",
    "# Optuna study\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=100, n_jobs=4)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best Hyperparameters:\", study.best_params)\n",
    "print(\"Best Validation Loss:\", study.best_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "class CNNModelWithEmbedding(nn.Module):\n",
    "    def __init__(self, input_dim, num_channels1, num_channels2, embedding_dim, num_cities, dropout_rate):\n",
    "        super(CNNModelWithEmbedding, self).__init__()\n",
    "        \n",
    "        # Embedding for city index\n",
    "        self.city_embedding = nn.Embedding(num_embeddings=num_cities, embedding_dim=embedding_dim)\n",
    "        \n",
    "        # CNN layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim + embedding_dim, out_channels=num_channels1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=num_channels1, out_channels=num_channels2, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(num_channels2 * (T // 2), 64)  # Adjust based on sequence length (T)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        self.output = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, x, city_idx):\n",
    "        # Add city embeddings\n",
    "        city_emb = self.city_embedding(city_idx).unsqueeze(1).expand(-1, x.size(1), -1)\n",
    "        x = torch.cat([x, city_emb], dim=2)\n",
    "        \n",
    "        # Permute for CNN (channels-first format)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # Pass through CNN layers\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        \n",
    "        # Flatten for fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "        x = self.dropout(self.fc1(x))\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(self.fc2(x))\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = X_train.shape[2]  # Number of features\n",
    "num_cities = city_train.max().item() + 1  # Total number of unique cities\n",
    "epochs = 1000\n",
    "batch_size = 64\n",
    "embedding_dim = 9 # Dimension of the embedding layer\n",
    "num_channels1=48\n",
    "num_channels2=80\n",
    "dropout_rate=0.25\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "model =  model = CNNModelWithEmbedding(\n",
    "            input_dim=X_train.shape[2],\n",
    "            num_channels1=num_channels1,\n",
    "            num_channels2=num_channels2,\n",
    "            embedding_dim=embedding_dim,\n",
    "            num_cities=city_train.max().item() + 1,\n",
    "            dropout_rate=dropout_rate\n",
    "        ).to(device).to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=7e-3, weight_decay=1e-5)\n",
    "\n",
    "# Convert data to PyTorch tensors and move to device\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "city_train_tensor = torch.tensor(city_train, dtype=torch.long).to(device)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "city_test_tensor = torch.tensor(city_test, dtype=torch.long).to(device)\n",
    "\n",
    "# DataLoader for batching\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor, city_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor, city_test_tensor)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Train Loss: 86.2398, Validation Loss: 7.9235\n",
      "Best model saved at epoch 1 with validation loss: 7.9235\n",
      "Epoch 6/1000, Train Loss: 23.0107, Validation Loss: 8.6021\n",
      "Epoch 11/1000, Train Loss: 22.5063, Validation Loss: 6.1876\n",
      "Best model saved at epoch 11 with validation loss: 6.1876\n",
      "Epoch 16/1000, Train Loss: 22.1113, Validation Loss: 3.4444\n",
      "Best model saved at epoch 16 with validation loss: 3.4444\n",
      "Epoch 21/1000, Train Loss: 21.9548, Validation Loss: 4.5751\n",
      "Epoch 26/1000, Train Loss: 22.1569, Validation Loss: 4.3184\n",
      "Epoch 31/1000, Train Loss: 21.8042, Validation Loss: 5.8662\n",
      "Epoch 36/1000, Train Loss: 21.2109, Validation Loss: 4.4837\n",
      "Epoch 41/1000, Train Loss: 19.3400, Validation Loss: 3.9116\n",
      "Epoch 46/1000, Train Loss: 19.2736, Validation Loss: 3.4771\n",
      "Epoch 51/1000, Train Loss: 19.3179, Validation Loss: 3.0226\n",
      "Best model saved at epoch 51 with validation loss: 3.0226\n",
      "Epoch 56/1000, Train Loss: 19.2099, Validation Loss: 4.8454\n",
      "Epoch 61/1000, Train Loss: 19.3394, Validation Loss: 3.8228\n",
      "Epoch 66/1000, Train Loss: 19.2540, Validation Loss: 3.4831\n",
      "Epoch 71/1000, Train Loss: 19.2150, Validation Loss: 4.0185\n",
      "Epoch 76/1000, Train Loss: 19.2409, Validation Loss: 3.7139\n",
      "Epoch 81/1000, Train Loss: 19.2786, Validation Loss: 3.5367\n",
      "Epoch 86/1000, Train Loss: 19.2560, Validation Loss: 4.9022\n",
      "Epoch 91/1000, Train Loss: 19.1273, Validation Loss: 2.9262\n",
      "Best model saved at epoch 91 with validation loss: 2.9262\n",
      "Epoch 96/1000, Train Loss: 19.1991, Validation Loss: 5.1633\n",
      "Epoch 101/1000, Train Loss: 19.0861, Validation Loss: 3.9201\n",
      "Epoch 106/1000, Train Loss: 19.2960, Validation Loss: 5.6313\n",
      "Epoch 111/1000, Train Loss: 19.0683, Validation Loss: 3.0184\n",
      "Epoch 116/1000, Train Loss: 19.3022, Validation Loss: 2.9891\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Initialize variables to track the best model\n",
    "best_val_loss = np.inf  # Set to infinity initially\n",
    "best_checkpoint_path = \"/home/research/a.naveen/denoise40/weather/cnn/best_model.pth\"\n",
    "\n",
    "train_losses=[]\n",
    "val_losses=[]\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch, city_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch, city_batch)\n",
    "        loss = loss_fn(output.squeeze(), y_batch.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    if epoch%5==0:\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch, city_batch in test_loader:\n",
    "                output = model(X_batch, city_batch)\n",
    "                loss = loss_fn(output.squeeze(), y_batch.squeeze())\n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        # Calculate average losses\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        val_loss /= len(test_loader.dataset)\n",
    "\n",
    "        # Append losses for plotting\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Save the model if validation loss improves\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss\n",
    "            }, best_checkpoint_path)\n",
    "            print(f\"Best model saved at epoch {epoch+1} with validation loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-01 14:23:55,474] A new study created in memory with name: no-name-e31246dd-7e35-4ff1-a481-26d3faaddeed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-01 14:26:27,714] Trial 2 finished with value: 1584.5580678185095 and parameters: {'embedding_dim': 15, 'lstm_hidden_dim': 64, 'num_channels1': 128, 'num_channels2': 240, 'dropout_rate': 0.3737043635986238, 'learning_rate': 0.0002572020852429956, 'batch_size': 256}. Best is trial 2 with value: 1584.5580678185095.\n",
      "[I 2024-12-01 14:26:34,919] Trial 1 finished with value: 340.3819314415565 and parameters: {'embedding_dim': 11, 'lstm_hidden_dim': 128, 'num_channels1': 128, 'num_channels2': 112, 'dropout_rate': 0.2707011995697526, 'learning_rate': 0.0087541747148776, 'batch_size': 64}. Best is trial 1 with value: 340.3819314415565.\n",
      "[I 2024-12-01 14:26:42,699] Trial 3 finished with value: 328.6687864896334 and parameters: {'embedding_dim': 5, 'lstm_hidden_dim': 256, 'num_channels1': 128, 'num_channels2': 256, 'dropout_rate': 0.2577330083500772, 'learning_rate': 0.0028869231158946958, 'batch_size': 128}. Best is trial 3 with value: 328.6687864896334.\n",
      "[I 2024-12-01 14:26:59,498] Trial 0 finished with value: 482.72812230318505 and parameters: {'embedding_dim': 13, 'lstm_hidden_dim': 64, 'num_channels1': 48, 'num_channels2': 160, 'dropout_rate': 0.35748461976075296, 'learning_rate': 0.00030962921032525755, 'batch_size': 64}. Best is trial 3 with value: 328.6687864896334.\n",
      "[I 2024-12-01 14:27:02,173] Trial 5 finished with value: 325.6879687894381 and parameters: {'embedding_dim': 8, 'lstm_hidden_dim': 128, 'num_channels1': 112, 'num_channels2': 208, 'dropout_rate': 0.39536688166969924, 'learning_rate': 0.0008499822647544035, 'batch_size': 128}. Best is trial 5 with value: 325.6879687894381.\n",
      "[I 2024-12-01 14:27:13,065] Trial 4 finished with value: 352.97057428354117 and parameters: {'embedding_dim': 15, 'lstm_hidden_dim': 64, 'num_channels1': 32, 'num_channels2': 256, 'dropout_rate': 0.44270796671806434, 'learning_rate': 0.005652499731316555, 'batch_size': 256}. Best is trial 5 with value: 325.6879687894381.\n",
      "[I 2024-12-01 14:27:20,116] Trial 8 finished with value: 480.6285689795861 and parameters: {'embedding_dim': 16, 'lstm_hidden_dim': 256, 'num_channels1': 48, 'num_channels2': 96, 'dropout_rate': 0.23857501072747297, 'learning_rate': 0.0001209903495765307, 'batch_size': 64}. Best is trial 5 with value: 325.6879687894381.\n",
      "[I 2024-12-01 14:27:23,530] Trial 7 finished with value: 354.2832292010968 and parameters: {'embedding_dim': 12, 'lstm_hidden_dim': 128, 'num_channels1': 48, 'num_channels2': 160, 'dropout_rate': 0.36090625653559183, 'learning_rate': 0.00753953670453297, 'batch_size': 64}. Best is trial 5 with value: 325.6879687894381.\n",
      "[I 2024-12-01 14:27:25,636] Trial 10 finished with value: 584.1067091468224 and parameters: {'embedding_dim': 6, 'lstm_hidden_dim': 256, 'num_channels1': 80, 'num_channels2': 112, 'dropout_rate': 0.47960274830041355, 'learning_rate': 0.000494694442769102, 'batch_size': 256}. Best is trial 5 with value: 325.6879687894381.\n",
      "[I 2024-12-01 14:27:28,253] Trial 9 finished with value: 340.76672881047176 and parameters: {'embedding_dim': 8, 'lstm_hidden_dim': 128, 'num_channels1': 48, 'num_channels2': 112, 'dropout_rate': 0.3049925140185601, 'learning_rate': 0.007730810121786667, 'batch_size': 256}. Best is trial 5 with value: 325.6879687894381.\n",
      "[I 2024-12-01 14:27:30,702] Trial 11 finished with value: 361.6694100792518 and parameters: {'embedding_dim': 10, 'lstm_hidden_dim': 256, 'num_channels1': 128, 'num_channels2': 192, 'dropout_rate': 0.4427785495342488, 'learning_rate': 0.009010073016321835, 'batch_size': 256}. Best is trial 5 with value: 325.6879687894381.\n",
      "[I 2024-12-01 14:27:31,501] Trial 6 finished with value: 314.5100425175593 and parameters: {'embedding_dim': 8, 'lstm_hidden_dim': 256, 'num_channels1': 80, 'num_channels2': 160, 'dropout_rate': 0.24138636985148954, 'learning_rate': 0.0004443763325582479, 'batch_size': 64}. Best is trial 6 with value: 314.5100425175593.\n",
      "[I 2024-12-01 14:27:32,162] Trial 12 finished with value: 314.2244655390813 and parameters: {'embedding_dim': 15, 'lstm_hidden_dim': 64, 'num_channels1': 128, 'num_channels2': 224, 'dropout_rate': 0.24650187406938243, 'learning_rate': 0.006454732513417743, 'batch_size': 256}. Best is trial 12 with value: 314.2244655390813.\n",
      "[I 2024-12-01 14:27:40,067] Trial 14 finished with value: 307.1224532404973 and parameters: {'embedding_dim': 4, 'lstm_hidden_dim': 128, 'num_channels1': 96, 'num_channels2': 208, 'dropout_rate': 0.20133691876054238, 'learning_rate': 0.002230240067869976, 'batch_size': 128}. Best is trial 14 with value: 307.1224532404973.\n",
      "[I 2024-12-01 14:27:46,683] Trial 16 finished with value: 322.4575721440242 and parameters: {'embedding_dim': 8, 'lstm_hidden_dim': 64, 'num_channels1': 96, 'num_channels2': 160, 'dropout_rate': 0.21207172884846806, 'learning_rate': 0.0018245855003441753, 'batch_size': 64}. Best is trial 14 with value: 307.1224532404973.\n",
      "[I 2024-12-01 14:27:48,680] Trial 15 finished with value: 333.29513722956733 and parameters: {'embedding_dim': 8, 'lstm_hidden_dim': 128, 'num_channels1': 96, 'num_channels2': 208, 'dropout_rate': 0.21369063064784338, 'learning_rate': 0.0011950732287576572, 'batch_size': 128}. Best is trial 14 with value: 307.1224532404973.\n",
      "[I 2024-12-01 14:27:49,619] Trial 17 finished with value: 312.9577384469839 and parameters: {'embedding_dim': 4, 'lstm_hidden_dim': 64, 'num_channels1': 96, 'num_channels2': 208, 'dropout_rate': 0.21179815241975322, 'learning_rate': 0.0019169074308743477, 'batch_size': 128}. Best is trial 14 with value: 307.1224532404973.\n",
      "[I 2024-12-01 14:27:55,768] Trial 18 finished with value: 344.28339703134395 and parameters: {'embedding_dim': 4, 'lstm_hidden_dim': 128, 'num_channels1': 96, 'num_channels2': 208, 'dropout_rate': 0.30346922373440444, 'learning_rate': 0.003280648928390656, 'batch_size': 128}. Best is trial 14 with value: 307.1224532404973.\n",
      "[I 2024-12-01 14:27:56,358] Trial 13 finished with value: 343.6295833453838 and parameters: {'embedding_dim': 8, 'lstm_hidden_dim': 128, 'num_channels1': 96, 'num_channels2': 208, 'dropout_rate': 0.4213372912651532, 'learning_rate': 0.0012290331355292146, 'batch_size': 128}. Best is trial 14 with value: 307.1224532404973.\n",
      "[I 2024-12-01 14:27:59,982] Trial 20 finished with value: 338.0881395470252 and parameters: {'embedding_dim': 4, 'lstm_hidden_dim': 128, 'num_channels1': 80, 'num_channels2': 64, 'dropout_rate': 0.295642938703765, 'learning_rate': 0.002938921268249052, 'batch_size': 128}. Best is trial 14 with value: 307.1224532404973.\n",
      "[I 2024-12-01 14:28:04,580] Trial 21 finished with value: 307.8994293851412 and parameters: {'embedding_dim': 6, 'lstm_hidden_dim': 64, 'num_channels1': 64, 'num_channels2': 64, 'dropout_rate': 0.3013702432147652, 'learning_rate': 0.0024291076710039983, 'batch_size': 128}. Best is trial 14 with value: 307.1224532404973.\n",
      "[I 2024-12-01 14:28:04,801] Trial 19 finished with value: 330.2189730731671 and parameters: {'embedding_dim': 4, 'lstm_hidden_dim': 64, 'num_channels1': 96, 'num_channels2': 224, 'dropout_rate': 0.3110355571132369, 'learning_rate': 0.0033302752712991773, 'batch_size': 128}. Best is trial 14 with value: 307.1224532404973.\n",
      "[I 2024-12-01 14:28:09,790] Trial 23 finished with value: 302.96524362722545 and parameters: {'embedding_dim': 6, 'lstm_hidden_dim': 64, 'num_channels1': 64, 'num_channels2': 176, 'dropout_rate': 0.20287656238477253, 'learning_rate': 0.001926250819831269, 'batch_size': 128}. Best is trial 23 with value: 302.96524362722545.\n",
      "[I 2024-12-01 14:28:13,439] Trial 24 finished with value: 319.60101210890554 and parameters: {'embedding_dim': 6, 'lstm_hidden_dim': 64, 'num_channels1': 64, 'num_channels2': 176, 'dropout_rate': 0.32419148859070385, 'learning_rate': 0.001997598719186546, 'batch_size': 128}. Best is trial 23 with value: 302.96524362722545.\n",
      "[I 2024-12-01 14:28:19,405] Trial 26 finished with value: 300.5351152573806 and parameters: {'embedding_dim': 6, 'lstm_hidden_dim': 64, 'num_channels1': 32, 'num_channels2': 176, 'dropout_rate': 0.2004179841086225, 'learning_rate': 0.0017995362835762328, 'batch_size': 128}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:28:20,476] Trial 25 finished with value: 328.2787324768066 and parameters: {'embedding_dim': 6, 'lstm_hidden_dim': 64, 'num_channels1': 64, 'num_channels2': 176, 'dropout_rate': 0.20302518329515443, 'learning_rate': 0.0019274215249425546, 'batch_size': 128}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:28:22,037] Trial 27 finished with value: 308.0006303663987 and parameters: {'embedding_dim': 6, 'lstm_hidden_dim': 64, 'num_channels1': 64, 'num_channels2': 64, 'dropout_rate': 0.2784703646876813, 'learning_rate': 0.004672454095367473, 'batch_size': 128}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:28:22,804] Trial 22 finished with value: 320.6091356548603 and parameters: {'embedding_dim': 6, 'lstm_hidden_dim': 64, 'num_channels1': 64, 'num_channels2': 176, 'dropout_rate': 0.3036532113717348, 'learning_rate': 0.0034021887531573533, 'batch_size': 128}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:28:29,811] Trial 28 finished with value: 336.5870689056397 and parameters: {'embedding_dim': 6, 'lstm_hidden_dim': 64, 'num_channels1': 32, 'num_channels2': 144, 'dropout_rate': 0.2014627607840389, 'learning_rate': 0.0008128806918555, 'batch_size': 128}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:28:31,139] Trial 30 finished with value: 324.1566903296837 and parameters: {'embedding_dim': 5, 'lstm_hidden_dim': 64, 'num_channels1': 32, 'num_channels2': 144, 'dropout_rate': 0.22699041739316048, 'learning_rate': 0.0010831691601363347, 'batch_size': 128}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:28:37,185] Trial 29 finished with value: 320.1795769066444 and parameters: {'embedding_dim': 5, 'lstm_hidden_dim': 64, 'num_channels1': 32, 'num_channels2': 144, 'dropout_rate': 0.27557463490084794, 'learning_rate': 0.004518016392807319, 'batch_size': 128}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:28:39,869] Trial 33 finished with value: 348.5795700974685 and parameters: {'embedding_dim': 9, 'lstm_hidden_dim': 128, 'num_channels1': 112, 'num_channels2': 192, 'dropout_rate': 0.3400863552062471, 'learning_rate': 0.0014248146528708084, 'batch_size': 128}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:28:40,119] Trial 32 finished with value: 315.60843756526066 and parameters: {'embedding_dim': 5, 'lstm_hidden_dim': 128, 'num_channels1': 112, 'num_channels2': 128, 'dropout_rate': 0.22957388343641805, 'learning_rate': 0.0014512643920681226, 'batch_size': 128}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:28:48,696] Trial 35 finished with value: 323.9309676358736 and parameters: {'embedding_dim': 7, 'lstm_hidden_dim': 64, 'num_channels1': 48, 'num_channels2': 192, 'dropout_rate': 0.22860258498362993, 'learning_rate': 0.0023057294935441724, 'batch_size': 128}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:28:50,100] Trial 36 finished with value: 312.2951866333008 and parameters: {'embedding_dim': 10, 'lstm_hidden_dim': 64, 'num_channels1': 48, 'num_channels2': 80, 'dropout_rate': 0.2568811907218644, 'learning_rate': 0.002459719427988736, 'batch_size': 128}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:28:50,665] Trial 31 finished with value: 356.77518223900427 and parameters: {'embedding_dim': 5, 'lstm_hidden_dim': 64, 'num_channels1': 32, 'num_channels2': 144, 'dropout_rate': 0.22926070033733137, 'learning_rate': 0.0007568163084831262, 'batch_size': 128}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:28:53,444] Trial 34 finished with value: 313.32015234281096 and parameters: {'embedding_dim': 7, 'lstm_hidden_dim': 64, 'num_channels1': 112, 'num_channels2': 192, 'dropout_rate': 0.3352031686974435, 'learning_rate': 0.0014465407715563224, 'batch_size': 128}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:28:57,470] Trial 37 finished with value: 431.72972434692383 and parameters: {'embedding_dim': 7, 'lstm_hidden_dim': 64, 'num_channels1': 80, 'num_channels2': 80, 'dropout_rate': 0.2563364808061473, 'learning_rate': 0.0008035695714766338, 'batch_size': 128}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:29:00,108] Trial 38 finished with value: 471.7843756080041 and parameters: {'embedding_dim': 7, 'lstm_hidden_dim': 64, 'num_channels1': 64, 'num_channels2': 240, 'dropout_rate': 0.2612402579614959, 'learning_rate': 0.0005810694789543252, 'batch_size': 128}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:29:11,640] Trial 41 finished with value: 348.86628380126956 and parameters: {'embedding_dim': 13, 'lstm_hidden_dim': 256, 'num_channels1': 64, 'num_channels2': 240, 'dropout_rate': 0.2765442376089624, 'learning_rate': 0.004366901065850975, 'batch_size': 64}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:29:16,936] Trial 42 finished with value: 314.35265345130335 and parameters: {'embedding_dim': 13, 'lstm_hidden_dim': 256, 'num_channels1': 64, 'num_channels2': 176, 'dropout_rate': 0.37034393544021105, 'learning_rate': 0.0041604921700542224, 'batch_size': 64}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:29:21,444] Trial 39 finished with value: 365.82905782118576 and parameters: {'embedding_dim': 7, 'lstm_hidden_dim': 64, 'num_channels1': 64, 'num_channels2': 224, 'dropout_rate': 0.26430568749050876, 'learning_rate': 0.0006187748804645663, 'batch_size': 128}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:29:24,586] Trial 40 finished with value: 308.66785216862604 and parameters: {'embedding_dim': 7, 'lstm_hidden_dim': 64, 'num_channels1': 64, 'num_channels2': 240, 'dropout_rate': 0.2623983415713776, 'learning_rate': 0.0005688809148089832, 'batch_size': 64}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:29:24,997] Trial 43 finished with value: 487.1775329176682 and parameters: {'embedding_dim': 9, 'lstm_hidden_dim': 128, 'num_channels1': 80, 'num_channels2': 176, 'dropout_rate': 0.38591921798801815, 'learning_rate': 0.00019909882917607116, 'batch_size': 64}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:29:25,923] Trial 44 finished with value: 307.37871960707446 and parameters: {'embedding_dim': 5, 'lstm_hidden_dim': 64, 'num_channels1': 80, 'num_channels2': 80, 'dropout_rate': 0.28701780314103326, 'learning_rate': 0.0024699654612125714, 'batch_size': 128}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:29:32,429] Trial 48 finished with value: 360.5459307025616 and parameters: {'embedding_dim': 5, 'lstm_hidden_dim': 64, 'num_channels1': 80, 'num_channels2': 96, 'dropout_rate': 0.2888447394183496, 'learning_rate': 0.0025086224492513907, 'batch_size': 256}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:29:34,349] Trial 47 finished with value: 340.5827557598408 and parameters: {'embedding_dim': 5, 'lstm_hidden_dim': 64, 'num_channels1': 48, 'num_channels2': 64, 'dropout_rate': 0.2874809026149673, 'learning_rate': 0.005436478597038726, 'batch_size': 128}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:29:41,806] Trial 46 finished with value: 303.7204038654034 and parameters: {'embedding_dim': 9, 'lstm_hidden_dim': 64, 'num_channels1': 80, 'num_channels2': 64, 'dropout_rate': 0.2864562588762824, 'learning_rate': 0.005682956187001931, 'batch_size': 128}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:29:42,352] Trial 49 finished with value: 316.68980263202377 and parameters: {'embedding_dim': 4, 'lstm_hidden_dim': 64, 'num_channels1': 80, 'num_channels2': 80, 'dropout_rate': 0.24248370893068572, 'learning_rate': 0.0016895698246009005, 'batch_size': 128}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:29:43,948] Trial 50 finished with value: 308.29415795428935 and parameters: {'embedding_dim': 12, 'lstm_hidden_dim': 128, 'num_channels1': 80, 'num_channels2': 80, 'dropout_rate': 0.24162566133036115, 'learning_rate': 0.0016095543073149964, 'batch_size': 128}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:29:48,561] Trial 45 finished with value: 310.191995298415 and parameters: {'embedding_dim': 9, 'lstm_hidden_dim': 64, 'num_channels1': 80, 'num_channels2': 64, 'dropout_rate': 0.28452926820056207, 'learning_rate': 0.005209415802256887, 'batch_size': 128}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:29:49,948] Trial 52 finished with value: 309.16719523268483 and parameters: {'embedding_dim': 11, 'lstm_hidden_dim': 128, 'num_channels1': 96, 'num_channels2': 96, 'dropout_rate': 0.21941128390265505, 'learning_rate': 0.009689323705667689, 'batch_size': 256}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:29:50,701] Trial 53 finished with value: 335.2267160630446 and parameters: {'embedding_dim': 4, 'lstm_hidden_dim': 256, 'num_channels1': 96, 'num_channels2': 160, 'dropout_rate': 0.3517859913662122, 'learning_rate': 0.009897346740083007, 'batch_size': 256}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:29:51,966] Trial 51 finished with value: 309.30961656705415 and parameters: {'embedding_dim': 9, 'lstm_hidden_dim': 128, 'num_channels1': 80, 'num_channels2': 96, 'dropout_rate': 0.24900435284307307, 'learning_rate': 0.006172550498547876, 'batch_size': 256}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:30:00,486] Trial 56 finished with value: 316.18213449284485 and parameters: {'embedding_dim': 6, 'lstm_hidden_dim': 64, 'num_channels1': 48, 'num_channels2': 128, 'dropout_rate': 0.31325453827238076, 'learning_rate': 0.006429363762249132, 'batch_size': 128}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:30:01,022] Trial 55 finished with value: 331.3986289438101 and parameters: {'embedding_dim': 4, 'lstm_hidden_dim': 64, 'num_channels1': 96, 'num_channels2': 128, 'dropout_rate': 0.3478414667617471, 'learning_rate': 0.006711387584119938, 'batch_size': 128}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:30:02,948] Trial 54 finished with value: 345.19412377929683 and parameters: {'embedding_dim': 11, 'lstm_hidden_dim': 64, 'num_channels1': 96, 'num_channels2': 96, 'dropout_rate': 0.21938894362259662, 'learning_rate': 0.0024210291375777286, 'batch_size': 256}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:30:08,931] Trial 57 finished with value: 322.2335444340633 and parameters: {'embedding_dim': 6, 'lstm_hidden_dim': 64, 'num_channels1': 48, 'num_channels2': 128, 'dropout_rate': 0.3186660204359548, 'learning_rate': 0.00371149790083687, 'batch_size': 128}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:30:09,698] Trial 58 finished with value: 307.0787808898925 and parameters: {'embedding_dim': 4, 'lstm_hidden_dim': 64, 'num_channels1': 96, 'num_channels2': 64, 'dropout_rate': 0.20113677341377098, 'learning_rate': 0.0027979217965642933, 'batch_size': 128}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:30:12,067] Trial 59 finished with value: 349.14798592810996 and parameters: {'embedding_dim': 6, 'lstm_hidden_dim': 64, 'num_channels1': 80, 'num_channels2': 64, 'dropout_rate': 0.4992819012263069, 'learning_rate': 0.003558339114684237, 'batch_size': 128}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:30:18,739] Trial 62 finished with value: 314.7714344834548 and parameters: {'embedding_dim': 4, 'lstm_hidden_dim': 256, 'num_channels1': 112, 'num_channels2': 112, 'dropout_rate': 0.20041573454338033, 'learning_rate': 0.002743430763405836, 'batch_size': 128}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:30:22,099] Trial 63 finished with value: 302.2452485450157 and parameters: {'embedding_dim': 4, 'lstm_hidden_dim': 256, 'num_channels1': 112, 'num_channels2': 80, 'dropout_rate': 0.2002712148208146, 'learning_rate': 0.0029063831570507086, 'batch_size': 128}. Best is trial 26 with value: 300.5351152573806.\n",
      "[I 2024-12-01 14:30:24,418] Trial 61 finished with value: 299.3201611961952 and parameters: {'embedding_dim': 5, 'lstm_hidden_dim': 64, 'num_channels1': 80, 'num_channels2': 64, 'dropout_rate': 0.20078875098427415, 'learning_rate': 0.002703238330335273, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:30:27,432] Trial 64 finished with value: 300.4691769538293 and parameters: {'embedding_dim': 5, 'lstm_hidden_dim': 64, 'num_channels1': 96, 'num_channels2': 80, 'dropout_rate': 0.21237580390518612, 'learning_rate': 0.002053739759755812, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:30:31,893] Trial 60 finished with value: 314.0763548551119 and parameters: {'embedding_dim': 6, 'lstm_hidden_dim': 64, 'num_channels1': 80, 'num_channels2': 64, 'dropout_rate': 0.20081992595500212, 'learning_rate': 0.0029336712275699825, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:30:33,311] Trial 65 finished with value: 313.03591959744966 and parameters: {'embedding_dim': 5, 'lstm_hidden_dim': 256, 'num_channels1': 128, 'num_channels2': 80, 'dropout_rate': 0.21093424951159206, 'learning_rate': 0.002140779126734099, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:30:37,093] Trial 67 finished with value: 324.6265601069524 and parameters: {'embedding_dim': 5, 'lstm_hidden_dim': 256, 'num_channels1': 128, 'num_channels2': 80, 'dropout_rate': 0.21403442178023835, 'learning_rate': 0.0030455313682093183, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:30:40,763] Trial 66 finished with value: 345.7888938960149 and parameters: {'embedding_dim': 4, 'lstm_hidden_dim': 256, 'num_channels1': 128, 'num_channels2': 64, 'dropout_rate': 0.21151963347312125, 'learning_rate': 0.0020805536490294822, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:30:43,492] Trial 69 finished with value: 302.55528423368014 and parameters: {'embedding_dim': 5, 'lstm_hidden_dim': 256, 'num_channels1': 112, 'num_channels2': 64, 'dropout_rate': 0.21486346491694966, 'learning_rate': 0.0011720346893323904, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:30:46,166] Trial 70 finished with value: 320.2349833181528 and parameters: {'embedding_dim': 4, 'lstm_hidden_dim': 256, 'num_channels1': 112, 'num_channels2': 64, 'dropout_rate': 0.2340314834727767, 'learning_rate': 0.0009566880580300898, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:30:53,157] Trial 72 finished with value: 322.51734765272874 and parameters: {'embedding_dim': 5, 'lstm_hidden_dim': 256, 'num_channels1': 112, 'num_channels2': 112, 'dropout_rate': 0.23364701130546078, 'learning_rate': 0.0009858754038781941, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:30:54,886] Trial 73 finished with value: 300.05856922372675 and parameters: {'embedding_dim': 8, 'lstm_hidden_dim': 256, 'num_channels1': 112, 'num_channels2': 96, 'dropout_rate': 0.22277191418170517, 'learning_rate': 0.0012187533489620814, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:30:56,646] Trial 71 finished with value: 300.8682830871582 and parameters: {'embedding_dim': 5, 'lstm_hidden_dim': 64, 'num_channels1': 112, 'num_channels2': 112, 'dropout_rate': 0.23645763067271924, 'learning_rate': 0.0009907184314482704, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:31:01,484] Trial 68 finished with value: 302.1839000352126 and parameters: {'embedding_dim': 5, 'lstm_hidden_dim': 256, 'num_channels1': 128, 'num_channels2': 80, 'dropout_rate': 0.21451793132001565, 'learning_rate': 0.0011896241527883363, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:31:03,743] Trial 74 finished with value: 301.53386139878495 and parameters: {'embedding_dim': 5, 'lstm_hidden_dim': 256, 'num_channels1': 112, 'num_channels2': 80, 'dropout_rate': 0.21730912094712493, 'learning_rate': 0.0012059194302290691, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:31:04,755] Trial 75 finished with value: 315.44104528292144 and parameters: {'embedding_dim': 5, 'lstm_hidden_dim': 256, 'num_channels1': 112, 'num_channels2': 80, 'dropout_rate': 0.22193528356121922, 'learning_rate': 0.0012311088618230993, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:31:13,260] Trial 79 finished with value: 313.3195675335224 and parameters: {'embedding_dim': 5, 'lstm_hidden_dim': 256, 'num_channels1': 128, 'num_channels2': 96, 'dropout_rate': 0.24927488267750372, 'learning_rate': 0.0012936348350453763, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:31:13,756] Trial 76 finished with value: 313.64604612285905 and parameters: {'embedding_dim': 5, 'lstm_hidden_dim': 256, 'num_channels1': 112, 'num_channels2': 96, 'dropout_rate': 0.22224056115110963, 'learning_rate': 0.0012228481731052405, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:31:13,980] Trial 78 finished with value: 303.45964743605396 and parameters: {'embedding_dim': 5, 'lstm_hidden_dim': 256, 'num_channels1': 128, 'num_channels2': 96, 'dropout_rate': 0.22348374322966993, 'learning_rate': 0.0012968074151835916, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:31:22,760] Trial 80 finished with value: 318.7485423037016 and parameters: {'embedding_dim': 8, 'lstm_hidden_dim': 256, 'num_channels1': 112, 'num_channels2': 112, 'dropout_rate': 0.2247575411995019, 'learning_rate': 0.0009202266087688506, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:31:24,240] Trial 82 finished with value: 331.48469042968753 and parameters: {'embedding_dim': 8, 'lstm_hidden_dim': 256, 'num_channels1': 112, 'num_channels2': 112, 'dropout_rate': 0.20879529300776828, 'learning_rate': 0.0009385449331534918, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:31:30,379] Trial 81 finished with value: 437.0249753725492 and parameters: {'embedding_dim': 8, 'lstm_hidden_dim': 256, 'num_channels1': 112, 'num_channels2': 112, 'dropout_rate': 0.20968724639275407, 'learning_rate': 0.00039373911310917436, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:31:30,598] Trial 77 finished with value: 315.7223880328839 and parameters: {'embedding_dim': 5, 'lstm_hidden_dim': 256, 'num_channels1': 112, 'num_channels2': 80, 'dropout_rate': 0.22206930471007508, 'learning_rate': 0.0011425187444408594, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:31:34,011] Trial 84 finished with value: 312.036308188101 and parameters: {'embedding_dim': 4, 'lstm_hidden_dim': 256, 'num_channels1': 112, 'num_channels2': 80, 'dropout_rate': 0.21526286994268096, 'learning_rate': 0.0006958904539646734, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:31:38,532] Trial 83 finished with value: 354.2477584435096 and parameters: {'embedding_dim': 16, 'lstm_hidden_dim': 256, 'num_channels1': 112, 'num_channels2': 80, 'dropout_rate': 0.4151014519455192, 'learning_rate': 0.0007025166775952658, 'batch_size': 64}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:31:46,971] Trial 88 finished with value: 316.57153447688177 and parameters: {'embedding_dim': 6, 'lstm_hidden_dim': 256, 'num_channels1': 128, 'num_channels2': 80, 'dropout_rate': 0.23579795623542496, 'learning_rate': 0.0017930661687796396, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:31:47,221] Trial 85 finished with value: 331.5247049480732 and parameters: {'embedding_dim': 6, 'lstm_hidden_dim': 256, 'num_channels1': 112, 'num_channels2': 80, 'dropout_rate': 0.23515837458060088, 'learning_rate': 0.0010678173082458522, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:31:52,751] Trial 87 finished with value: 321.72042933255705 and parameters: {'embedding_dim': 6, 'lstm_hidden_dim': 256, 'num_channels1': 128, 'num_channels2': 80, 'dropout_rate': 0.23595169212588452, 'learning_rate': 0.0016489846153439165, 'batch_size': 64}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:31:55,864] Trial 89 finished with value: 299.3309656301645 and parameters: {'embedding_dim': 6, 'lstm_hidden_dim': 256, 'num_channels1': 112, 'num_channels2': 96, 'dropout_rate': 0.20807764364133782, 'learning_rate': 0.0015667375400414995, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:31:59,730] Trial 86 finished with value: 318.54163037625824 and parameters: {'embedding_dim': 6, 'lstm_hidden_dim': 256, 'num_channels1': 112, 'num_channels2': 80, 'dropout_rate': 0.23245135862483302, 'learning_rate': 0.0017369056413618967, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:32:03,058] Trial 91 finished with value: 320.52842255436826 and parameters: {'embedding_dim': 7, 'lstm_hidden_dim': 256, 'num_channels1': 96, 'num_channels2': 96, 'dropout_rate': 0.20994384536906732, 'learning_rate': 0.0014425323004672295, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:32:04,109] Trial 90 finished with value: 304.5337254295936 and parameters: {'embedding_dim': 7, 'lstm_hidden_dim': 256, 'num_channels1': 128, 'num_channels2': 96, 'dropout_rate': 0.2081488565823793, 'learning_rate': 0.0015659716695729188, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:32:04,862] Trial 92 finished with value: 316.323396481558 and parameters: {'embedding_dim': 7, 'lstm_hidden_dim': 256, 'num_channels1': 96, 'num_channels2': 96, 'dropout_rate': 0.20895840913117267, 'learning_rate': 0.0015132344483744615, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:32:14,325] Trial 94 finished with value: 305.23711845773545 and parameters: {'embedding_dim': 4, 'lstm_hidden_dim': 256, 'num_channels1': 112, 'num_channels2': 96, 'dropout_rate': 0.20761982547026614, 'learning_rate': 0.0015392539124984578, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:32:14,494] Trial 96 finished with value: 328.3779933119554 and parameters: {'embedding_dim': 4, 'lstm_hidden_dim': 256, 'num_channels1': 112, 'num_channels2': 64, 'dropout_rate': 0.2503605893945155, 'learning_rate': 0.0010437421315267223, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:32:21,044] Trial 95 finished with value: 336.85272137310324 and parameters: {'embedding_dim': 4, 'lstm_hidden_dim': 256, 'num_channels1': 112, 'num_channels2': 64, 'dropout_rate': 0.2167588038816539, 'learning_rate': 0.001073898720861743, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:32:23,778] Trial 98 finished with value: 303.655255287992 and parameters: {'embedding_dim': 5, 'lstm_hidden_dim': 256, 'num_channels1': 112, 'num_channels2': 64, 'dropout_rate': 0.21750830198011042, 'learning_rate': 0.0013430677917636493, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:32:24,292] Trial 97 finished with value: 308.1173940645658 and parameters: {'embedding_dim': 5, 'lstm_hidden_dim': 256, 'num_channels1': 112, 'num_channels2': 64, 'dropout_rate': 0.21722058178601875, 'learning_rate': 0.001103917878206537, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:32:25,180] Trial 93 finished with value: 304.8948857079139 and parameters: {'embedding_dim': 7, 'lstm_hidden_dim': 256, 'num_channels1': 96, 'num_channels2': 96, 'dropout_rate': 0.20705033377538676, 'learning_rate': 0.0014906474592537291, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n",
      "[I 2024-12-01 14:32:28,314] Trial 99 finished with value: 314.1212762437087 and parameters: {'embedding_dim': 5, 'lstm_hidden_dim': 256, 'num_channels1': 96, 'num_channels2': 64, 'dropout_rate': 0.2421801046278636, 'learning_rate': 0.0008574202103368966, 'batch_size': 128}. Best is trial 61 with value: 299.3201611961952.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'embedding_dim': 5, 'lstm_hidden_dim': 64, 'num_channels1': 80, 'num_channels2': 64, 'dropout_rate': 0.20078875098427415, 'learning_rate': 0.002703238330335273, 'batch_size': 128}\n",
      "Best Validation Loss: 299.3201611961952\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import numpy as np\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "class HybridCNNLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, lstm_hidden_dim, num_channels1, num_channels2, num_cities, dropout_rate):\n",
    "        super(HybridCNNLSTM, self).__init__()\n",
    "        \n",
    "        # Embedding for city index\n",
    "        self.city_embedding = nn.Embedding(num_embeddings=num_cities, embedding_dim=embedding_dim)\n",
    "        \n",
    "        # CNN layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim + embedding_dim, out_channels=num_channels1,  kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=num_channels1, out_channels=num_channels2, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=num_channels2, hidden_size=lstm_hidden_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(lstm_hidden_dim * 2, 64)  # Bidirectional LSTM output\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        self.output = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, x, city_idx):\n",
    "        # Add city embeddings\n",
    "        city_emb = self.city_embedding(city_idx).unsqueeze(1).expand(-1, x.size(1), -1)\n",
    "        x = torch.cat([x, city_emb], dim=2)\n",
    "        \n",
    "        # Permute for CNN (channels-first format)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # Pass through CNN layers\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        \n",
    "        # Permute back for LSTM (batch-first format)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]  # Take the output from the last timestep\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "        x = self.dropout(self.fc1(x))\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(self.fc2(x))\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Sample hyperparameters\n",
    "    embedding_dim = trial.suggest_int(\"embedding_dim\", 4, 16)\n",
    "    lstm_hidden_dim = trial.suggest_categorical(\"lstm_hidden_dim\", [64, 128, 256])\n",
    "    num_channels1 = trial.suggest_int(\"num_channels1\", 32, 128, step=16)\n",
    "    num_channels2 = trial.suggest_int(\"num_channels2\", 64, 256, step=16)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.2, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
    "\n",
    "    n_splits_per_city = 5\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits_per_city)\n",
    "\n",
    "    # Store city-specific splits\n",
    "    city_splits = []\n",
    "\n",
    "    for city in city_train.unique().tolist():  # Get unique city IDs\n",
    "        city_mask = (city_train == city)\n",
    "        city_X = X_train[city_mask]\n",
    "        city_y = y_train[city_mask]\n",
    "        \n",
    "        # Subsample 50% of the data for faster tuning\n",
    "        subsample_size = int(0.1 * len(city_X))\n",
    "        subsample_indices = torch.arange(len(city_X))[:subsample_size]\n",
    "        city_X = city_X[subsample_indices]\n",
    "        city_y = city_y[subsample_indices]\n",
    "\n",
    "        # Split city data temporally\n",
    "        for train_idx, val_idx in tscv.split(city_X):\n",
    "            city_splits.append((\n",
    "                city_X[train_idx], city_X[val_idx],\n",
    "                city_y[train_idx], city_y[val_idx],\n",
    "                torch.full((len(train_idx),), city, dtype=torch.long),\n",
    "                torch.full((len(val_idx),), city, dtype=torch.long)\n",
    "            ))\n",
    "\n",
    "    # Combine city-specific splits into global folds\n",
    "    n_global_folds = 5\n",
    "    global_folds = [[] for _ in range(n_global_folds)]\n",
    "\n",
    "    for i, split in enumerate(city_splits):\n",
    "        global_folds[i % n_global_folds].append(split)\n",
    "\n",
    "    # Combine splits within each global fold\n",
    "    combined_folds = []\n",
    "    for fold in global_folds:\n",
    "        X_train_fold = torch.cat([f[0] for f in fold])\n",
    "        X_val_fold = torch.cat([f[1] for f in fold])\n",
    "        y_train_fold = torch.cat([f[2] for f in fold])\n",
    "        y_val_fold = torch.cat([f[3] for f in fold])\n",
    "        city_train_fold = torch.cat([f[4] for f in fold])\n",
    "        city_val_fold = torch.cat([f[5] for f in fold])\n",
    "\n",
    "        combined_folds.append((X_train_fold, X_val_fold, y_train_fold, y_val_fold, city_train_fold, city_val_fold))\n",
    "\n",
    "    # Perform training and validation\n",
    "    fold_val_losses = []\n",
    "\n",
    "    for X_train_fold, X_val_fold, y_train_fold, y_val_fold, city_train_fold, city_val_fold in combined_folds:\n",
    "        train_dataset = torch.utils.data.TensorDataset(X_train_fold, y_train_fold, city_train_fold)\n",
    "        val_dataset = torch.utils.data.TensorDataset(X_val_fold, y_val_fold, city_val_fold)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Initialize model\n",
    "        model = HybridCNNLSTM(\n",
    "            input_dim=X_train.shape[2],\n",
    "            embedding_dim=embedding_dim,\n",
    "            lstm_hidden_dim=lstm_hidden_dim,\n",
    "            num_channels1=num_channels1,\n",
    "            num_channels2=num_channels2,\n",
    "            num_cities=city_train.max().item() + 1,\n",
    "            dropout_rate=dropout_rate\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        loss_fn = nn.MSELoss()\n",
    "\n",
    "        # Train\n",
    "        for epoch in range(5):  # Use fewer epochs for tuning\n",
    "            model.train()\n",
    "            for X_batch, y_batch, city_batch in train_loader:\n",
    "                X_batch, y_batch, city_batch = X_batch.to(device), y_batch.to(device), city_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(X_batch, city_batch).squeeze()\n",
    "                loss = loss_fn(output, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch, city_batch in val_loader:\n",
    "                X_batch, y_batch, city_batch = X_batch.to(device), y_batch.to(device), city_batch.to(device)\n",
    "                output = model(X_batch, city_batch).squeeze()\n",
    "                loss = loss_fn(output, y_batch)\n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        fold_val_losses.append(val_loss)\n",
    "\n",
    "    avg_val_loss = np.mean(fold_val_losses)\n",
    "    return avg_val_loss\n",
    "\n",
    "\n",
    "# Optuna study\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=100, n_jobs=4)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best Hyperparameters:\", study.best_params)\n",
    "print(\"Best Validation Loss:\", study.best_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "class HybridCNNLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, lstm_hidden_dim, num_channels1, num_channels2, num_cities, dropout_rate):\n",
    "        super(HybridCNNLSTM, self).__init__()\n",
    "        \n",
    "        # Embedding for city index\n",
    "        self.city_embedding = nn.Embedding(num_embeddings=num_cities, embedding_dim=embedding_dim)\n",
    "        \n",
    "        # CNN layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim + embedding_dim, out_channels=num_channels1,  kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=num_channels1, out_channels=num_channels2, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=num_channels2, hidden_size=lstm_hidden_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(lstm_hidden_dim * 2, 64)  # Bidirectional LSTM output\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        self.output = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, x, city_idx):\n",
    "        # Add city embeddings\n",
    "        city_emb = self.city_embedding(city_idx).unsqueeze(1).expand(-1, x.size(1), -1)\n",
    "        x = torch.cat([x, city_emb], dim=2)\n",
    "        \n",
    "        # Permute for CNN (channels-first format)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # Pass through CNN layers\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        \n",
    "        # Permute back for LSTM (batch-first format)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]  # Take the output from the last timestep\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "        x = self.dropout(self.fc1(x))\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(self.fc2(x))\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = X_train.shape[2]  # Number of features\n",
    "num_cities = city_train.max().item() + 1  # Total number of unique cities\n",
    "epochs = 1000\n",
    "batch_size = 128\n",
    "embedding_dim = 5 # Dimension of the embedding layer\n",
    "num_channels1=48\n",
    "num_channels2=80\n",
    "dropout_rate=0.2\n",
    "lstm_hidden_dim=128\n",
    "lr=2e-3\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "model = HybridCNNLSTM(\n",
    "            input_dim=X_train.shape[2],\n",
    "            embedding_dim=embedding_dim,\n",
    "            lstm_hidden_dim=lstm_hidden_dim,\n",
    "            num_channels1=num_channels1,\n",
    "            num_channels2=num_channels2,\n",
    "            num_cities=city_train.max().item() + 1,\n",
    "            dropout_rate=dropout_rate\n",
    "        ).to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "# Convert data to PyTorch tensors and move to device\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "city_train_tensor = torch.tensor(city_train, dtype=torch.long).to(device)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "city_test_tensor = torch.tensor(city_test, dtype=torch.long).to(device)\n",
    "\n",
    "# DataLoader for batching\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor, city_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor, city_test_tensor)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Train Loss: 142.7550, Validation Loss: 7.4418\n",
      "Best model saved at epoch 1 with validation loss: 7.4418\n",
      "Epoch 6/1000, Train Loss: 17.2835, Validation Loss: 2.8043\n",
      "Best model saved at epoch 6 with validation loss: 2.8043\n",
      "Epoch 11/1000, Train Loss: 14.9947, Validation Loss: 2.4815\n",
      "Best model saved at epoch 11 with validation loss: 2.4815\n",
      "Epoch 16/1000, Train Loss: 14.9034, Validation Loss: 2.2871\n",
      "Best model saved at epoch 16 with validation loss: 2.2871\n",
      "Epoch 21/1000, Train Loss: 14.7778, Validation Loss: 2.5742\n",
      "Epoch 26/1000, Train Loss: 14.6351, Validation Loss: 2.5518\n",
      "Epoch 31/1000, Train Loss: 14.7007, Validation Loss: 2.3480\n",
      "Epoch 36/1000, Train Loss: 14.7339, Validation Loss: 2.1894\n",
      "Best model saved at epoch 36 with validation loss: 2.1894\n",
      "Epoch 41/1000, Train Loss: 14.7130, Validation Loss: 2.4523\n",
      "Epoch 46/1000, Train Loss: 14.5241, Validation Loss: 2.1677\n",
      "Best model saved at epoch 46 with validation loss: 2.1677\n",
      "Epoch 51/1000, Train Loss: 14.5378, Validation Loss: 2.2823\n",
      "Epoch 56/1000, Train Loss: 14.5349, Validation Loss: 2.3530\n",
      "Epoch 61/1000, Train Loss: 14.5129, Validation Loss: 2.5530\n",
      "Epoch 66/1000, Train Loss: 14.5386, Validation Loss: 2.3788\n",
      "Epoch 71/1000, Train Loss: 14.5157, Validation Loss: 2.0820\n",
      "Best model saved at epoch 71 with validation loss: 2.0820\n",
      "Epoch 76/1000, Train Loss: 14.5516, Validation Loss: 2.3816\n",
      "Epoch 81/1000, Train Loss: 14.4410, Validation Loss: 2.2283\n",
      "Epoch 86/1000, Train Loss: 14.5950, Validation Loss: 2.1434\n",
      "Epoch 91/1000, Train Loss: 14.5103, Validation Loss: 2.0328\n",
      "Best model saved at epoch 91 with validation loss: 2.0328\n",
      "Epoch 96/1000, Train Loss: 14.4554, Validation Loss: 2.6568\n",
      "Epoch 101/1000, Train Loss: 14.4729, Validation Loss: 1.9645\n",
      "Best model saved at epoch 101 with validation loss: 1.9645\n",
      "Epoch 106/1000, Train Loss: 14.5072, Validation Loss: 1.9742\n",
      "Epoch 111/1000, Train Loss: 14.5108, Validation Loss: 2.2981\n",
      "Epoch 116/1000, Train Loss: 14.4976, Validation Loss: 2.3542\n",
      "Epoch 121/1000, Train Loss: 14.4849, Validation Loss: 2.1022\n",
      "Epoch 126/1000, Train Loss: 14.5182, Validation Loss: 2.1049\n",
      "Epoch 131/1000, Train Loss: 14.4727, Validation Loss: 2.0172\n",
      "Epoch 136/1000, Train Loss: 14.4873, Validation Loss: 2.5796\n",
      "Epoch 141/1000, Train Loss: 14.4748, Validation Loss: 3.0745\n",
      "Epoch 151/1000, Train Loss: 14.4772, Validation Loss: 2.0362\n",
      "Epoch 156/1000, Train Loss: 14.4552, Validation Loss: 2.2628\n",
      "Epoch 161/1000, Train Loss: 14.4214, Validation Loss: 1.9909\n",
      "Epoch 166/1000, Train Loss: 14.3916, Validation Loss: 2.1661\n",
      "Epoch 171/1000, Train Loss: 14.4062, Validation Loss: 2.5570\n",
      "Epoch 176/1000, Train Loss: 14.4475, Validation Loss: 2.2128\n",
      "Epoch 181/1000, Train Loss: 14.4411, Validation Loss: 2.5991\n",
      "Epoch 186/1000, Train Loss: 14.4581, Validation Loss: 2.2195\n",
      "Epoch 191/1000, Train Loss: 14.4038, Validation Loss: 2.6577\n",
      "Epoch 196/1000, Train Loss: 14.4302, Validation Loss: 1.9806\n",
      "Epoch 201/1000, Train Loss: 14.4402, Validation Loss: 2.1071\n",
      "Epoch 206/1000, Train Loss: 14.4574, Validation Loss: 2.5438\n",
      "Epoch 211/1000, Train Loss: 14.3806, Validation Loss: 2.6995\n",
      "Epoch 216/1000, Train Loss: 14.4227, Validation Loss: 2.4620\n",
      "Epoch 221/1000, Train Loss: 14.3956, Validation Loss: 2.1267\n",
      "Epoch 226/1000, Train Loss: 14.4318, Validation Loss: 2.0912\n",
      "Epoch 231/1000, Train Loss: 14.4219, Validation Loss: 2.7306\n",
      "Epoch 236/1000, Train Loss: 14.4452, Validation Loss: 1.9587\n",
      "Best model saved at epoch 236 with validation loss: 1.9587\n",
      "Epoch 241/1000, Train Loss: 14.3889, Validation Loss: 2.0681\n",
      "Epoch 246/1000, Train Loss: 14.5023, Validation Loss: 2.4845\n",
      "Epoch 251/1000, Train Loss: 14.4269, Validation Loss: 1.9748\n",
      "Epoch 256/1000, Train Loss: 14.3926, Validation Loss: 3.1245\n",
      "Epoch 261/1000, Train Loss: 14.4049, Validation Loss: 2.2292\n",
      "Epoch 266/1000, Train Loss: 14.4440, Validation Loss: 2.4356\n",
      "Epoch 271/1000, Train Loss: 14.4004, Validation Loss: 2.1699\n",
      "Epoch 276/1000, Train Loss: 14.4562, Validation Loss: 2.1553\n",
      "Epoch 281/1000, Train Loss: 14.4418, Validation Loss: 2.3441\n",
      "Epoch 286/1000, Train Loss: 14.4170, Validation Loss: 2.1516\n",
      "Epoch 291/1000, Train Loss: 14.4065, Validation Loss: 2.3294\n",
      "Epoch 296/1000, Train Loss: 14.3768, Validation Loss: 2.3066\n",
      "Epoch 301/1000, Train Loss: 14.5139, Validation Loss: 2.7907\n",
      "Epoch 306/1000, Train Loss: 14.4456, Validation Loss: 2.3729\n",
      "Epoch 311/1000, Train Loss: 14.4180, Validation Loss: 2.4028\n",
      "Epoch 316/1000, Train Loss: 14.4037, Validation Loss: 1.9018\n",
      "Best model saved at epoch 316 with validation loss: 1.9018\n",
      "Epoch 321/1000, Train Loss: 14.4148, Validation Loss: 2.1943\n",
      "Epoch 326/1000, Train Loss: 14.3810, Validation Loss: 2.2410\n",
      "Epoch 331/1000, Train Loss: 14.4683, Validation Loss: 2.4786\n",
      "Epoch 336/1000, Train Loss: 14.4233, Validation Loss: 2.1457\n",
      "Epoch 341/1000, Train Loss: 14.4243, Validation Loss: 2.2418\n",
      "Epoch 346/1000, Train Loss: 14.3865, Validation Loss: 2.1630\n",
      "Epoch 351/1000, Train Loss: 14.5648, Validation Loss: 1.9487\n",
      "Epoch 356/1000, Train Loss: 14.3150, Validation Loss: 2.1482\n",
      "Epoch 361/1000, Train Loss: 14.3992, Validation Loss: 2.4760\n",
      "Epoch 366/1000, Train Loss: 14.3408, Validation Loss: 2.2215\n",
      "Epoch 371/1000, Train Loss: 14.3846, Validation Loss: 2.1727\n",
      "Epoch 376/1000, Train Loss: 14.3250, Validation Loss: 2.1830\n",
      "Epoch 381/1000, Train Loss: 14.4204, Validation Loss: 2.0605\n",
      "Epoch 386/1000, Train Loss: 14.3323, Validation Loss: 2.1983\n",
      "Epoch 391/1000, Train Loss: 14.3234, Validation Loss: 1.9883\n",
      "Epoch 396/1000, Train Loss: 14.3371, Validation Loss: 1.9500\n",
      "Epoch 401/1000, Train Loss: 14.4113, Validation Loss: 2.2948\n",
      "Epoch 406/1000, Train Loss: 14.3425, Validation Loss: 1.9716\n",
      "Epoch 411/1000, Train Loss: 14.3682, Validation Loss: 2.6776\n",
      "Epoch 416/1000, Train Loss: 14.4362, Validation Loss: 2.0279\n",
      "Epoch 421/1000, Train Loss: 14.3293, Validation Loss: 2.2906\n",
      "Epoch 426/1000, Train Loss: 14.3989, Validation Loss: 2.4483\n",
      "Epoch 431/1000, Train Loss: 14.4229, Validation Loss: 1.8609\n",
      "Best model saved at epoch 431 with validation loss: 1.8609\n",
      "Epoch 436/1000, Train Loss: 14.4080, Validation Loss: 2.2376\n",
      "Epoch 441/1000, Train Loss: 14.3214, Validation Loss: 2.2507\n",
      "Epoch 446/1000, Train Loss: 14.3939, Validation Loss: 1.8533\n",
      "Best model saved at epoch 446 with validation loss: 1.8533\n",
      "Epoch 451/1000, Train Loss: 14.3697, Validation Loss: 1.9825\n",
      "Epoch 456/1000, Train Loss: 14.3342, Validation Loss: 2.2849\n",
      "Epoch 461/1000, Train Loss: 14.3760, Validation Loss: 2.4388\n",
      "Epoch 466/1000, Train Loss: 14.3975, Validation Loss: 2.2372\n",
      "Epoch 471/1000, Train Loss: 14.3936, Validation Loss: 2.1837\n",
      "Epoch 476/1000, Train Loss: 14.3561, Validation Loss: 2.1875\n",
      "Epoch 481/1000, Train Loss: 14.3241, Validation Loss: 1.8897\n",
      "Epoch 486/1000, Train Loss: 14.3625, Validation Loss: 2.6689\n",
      "Epoch 491/1000, Train Loss: 14.3723, Validation Loss: 1.9229\n",
      "Epoch 496/1000, Train Loss: 14.3345, Validation Loss: 2.0045\n",
      "Epoch 501/1000, Train Loss: 14.3423, Validation Loss: 2.0508\n",
      "Epoch 506/1000, Train Loss: 14.4257, Validation Loss: 2.1249\n",
      "Epoch 511/1000, Train Loss: 14.3581, Validation Loss: 2.1744\n",
      "Epoch 516/1000, Train Loss: 14.3696, Validation Loss: 1.8830\n",
      "Epoch 521/1000, Train Loss: 14.3576, Validation Loss: 1.9729\n",
      "Epoch 526/1000, Train Loss: 14.3540, Validation Loss: 2.4297\n",
      "Epoch 531/1000, Train Loss: 14.3320, Validation Loss: 2.1925\n",
      "Epoch 536/1000, Train Loss: 14.3752, Validation Loss: 2.0740\n",
      "Epoch 541/1000, Train Loss: 14.4700, Validation Loss: 2.4248\n",
      "Epoch 546/1000, Train Loss: 14.3583, Validation Loss: 2.3641\n",
      "Epoch 551/1000, Train Loss: 14.3080, Validation Loss: 1.8941\n",
      "Epoch 556/1000, Train Loss: 14.4238, Validation Loss: 1.9938\n",
      "Epoch 561/1000, Train Loss: 14.3423, Validation Loss: 2.8897\n",
      "Epoch 566/1000, Train Loss: 14.3480, Validation Loss: 2.3287\n",
      "Epoch 571/1000, Train Loss: 14.3678, Validation Loss: 2.2348\n",
      "Epoch 576/1000, Train Loss: 14.3386, Validation Loss: 2.1922\n",
      "Epoch 581/1000, Train Loss: 14.3049, Validation Loss: 2.3783\n",
      "Epoch 586/1000, Train Loss: 14.3775, Validation Loss: 2.2694\n",
      "Epoch 591/1000, Train Loss: 14.4149, Validation Loss: 2.3346\n",
      "Epoch 596/1000, Train Loss: 14.3764, Validation Loss: 1.8745\n",
      "Epoch 601/1000, Train Loss: 14.2867, Validation Loss: 2.3383\n",
      "Epoch 606/1000, Train Loss: 14.3344, Validation Loss: 1.9992\n",
      "Epoch 611/1000, Train Loss: 14.3339, Validation Loss: 1.9921\n",
      "Epoch 616/1000, Train Loss: 14.3903, Validation Loss: 2.2266\n",
      "Epoch 621/1000, Train Loss: 14.3764, Validation Loss: 2.1831\n",
      "Epoch 626/1000, Train Loss: 14.3791, Validation Loss: 2.3640\n",
      "Epoch 631/1000, Train Loss: 14.3460, Validation Loss: 2.1222\n",
      "Epoch 636/1000, Train Loss: 14.3354, Validation Loss: 2.3987\n",
      "Epoch 641/1000, Train Loss: 14.2947, Validation Loss: 2.2041\n",
      "Epoch 646/1000, Train Loss: 14.3277, Validation Loss: 2.4223\n",
      "Epoch 651/1000, Train Loss: 14.3544, Validation Loss: 2.4646\n",
      "Epoch 656/1000, Train Loss: 14.3497, Validation Loss: 1.9223\n",
      "Epoch 661/1000, Train Loss: 14.3544, Validation Loss: 1.8996\n",
      "Epoch 666/1000, Train Loss: 14.3471, Validation Loss: 2.0765\n",
      "Epoch 671/1000, Train Loss: 14.3619, Validation Loss: 2.3764\n",
      "Epoch 676/1000, Train Loss: 14.3634, Validation Loss: 2.0566\n",
      "Epoch 681/1000, Train Loss: 14.3879, Validation Loss: 2.7478\n",
      "Epoch 686/1000, Train Loss: 14.3540, Validation Loss: 1.9901\n",
      "Epoch 691/1000, Train Loss: 14.2860, Validation Loss: 2.1865\n",
      "Epoch 696/1000, Train Loss: 14.3661, Validation Loss: 2.1773\n",
      "Epoch 701/1000, Train Loss: 14.3580, Validation Loss: 2.4983\n",
      "Epoch 706/1000, Train Loss: 14.3821, Validation Loss: 2.3527\n",
      "Epoch 711/1000, Train Loss: 14.3535, Validation Loss: 1.9833\n",
      "Epoch 716/1000, Train Loss: 14.4338, Validation Loss: 1.9692\n",
      "Epoch 721/1000, Train Loss: 14.3039, Validation Loss: 2.1597\n",
      "Epoch 726/1000, Train Loss: 14.4107, Validation Loss: 2.2582\n",
      "Epoch 731/1000, Train Loss: 14.3565, Validation Loss: 2.3033\n",
      "Epoch 736/1000, Train Loss: 14.3879, Validation Loss: 2.5490\n",
      "Epoch 741/1000, Train Loss: 14.3294, Validation Loss: 2.0487\n",
      "Epoch 746/1000, Train Loss: 14.3210, Validation Loss: 2.1468\n",
      "Epoch 751/1000, Train Loss: 14.3453, Validation Loss: 2.5771\n",
      "Epoch 756/1000, Train Loss: 14.3033, Validation Loss: 2.2207\n",
      "Epoch 761/1000, Train Loss: 14.3862, Validation Loss: 2.1490\n",
      "Epoch 766/1000, Train Loss: 14.3649, Validation Loss: 2.1539\n",
      "Epoch 771/1000, Train Loss: 14.3816, Validation Loss: 1.9421\n",
      "Epoch 776/1000, Train Loss: 14.3939, Validation Loss: 2.3362\n",
      "Epoch 781/1000, Train Loss: 14.4104, Validation Loss: 1.8923\n",
      "Epoch 786/1000, Train Loss: 14.2927, Validation Loss: 2.8682\n",
      "Epoch 791/1000, Train Loss: 14.3440, Validation Loss: 2.1094\n",
      "Epoch 796/1000, Train Loss: 14.3881, Validation Loss: 1.9279\n",
      "Epoch 801/1000, Train Loss: 14.4127, Validation Loss: 1.8292\n",
      "Best model saved at epoch 801 with validation loss: 1.8292\n",
      "Epoch 806/1000, Train Loss: 14.3649, Validation Loss: 2.0725\n",
      "Epoch 811/1000, Train Loss: 14.3381, Validation Loss: 2.5773\n",
      "Epoch 816/1000, Train Loss: 14.3428, Validation Loss: 1.9891\n",
      "Epoch 821/1000, Train Loss: 14.3567, Validation Loss: 1.9307\n",
      "Epoch 826/1000, Train Loss: 14.3061, Validation Loss: 2.8051\n",
      "Epoch 831/1000, Train Loss: 14.3916, Validation Loss: 2.2393\n",
      "Epoch 836/1000, Train Loss: 14.4014, Validation Loss: 2.1314\n",
      "Epoch 841/1000, Train Loss: 14.3420, Validation Loss: 2.5109\n",
      "Epoch 846/1000, Train Loss: 14.2854, Validation Loss: 2.1499\n",
      "Epoch 851/1000, Train Loss: 14.3722, Validation Loss: 1.9419\n",
      "Epoch 856/1000, Train Loss: 14.3501, Validation Loss: 2.1299\n",
      "Epoch 861/1000, Train Loss: 14.3899, Validation Loss: 2.5411\n",
      "Epoch 866/1000, Train Loss: 14.2971, Validation Loss: 2.1318\n",
      "Epoch 871/1000, Train Loss: 14.3817, Validation Loss: 1.9308\n",
      "Epoch 876/1000, Train Loss: 14.3511, Validation Loss: 1.9836\n",
      "Epoch 881/1000, Train Loss: 14.3391, Validation Loss: 2.1138\n",
      "Epoch 886/1000, Train Loss: 14.3611, Validation Loss: 3.0130\n",
      "Epoch 891/1000, Train Loss: 14.2984, Validation Loss: 2.0924\n",
      "Epoch 896/1000, Train Loss: 14.3874, Validation Loss: 2.5506\n",
      "Epoch 901/1000, Train Loss: 14.3163, Validation Loss: 2.3228\n",
      "Epoch 906/1000, Train Loss: 14.3744, Validation Loss: 2.0297\n",
      "Epoch 911/1000, Train Loss: 14.3368, Validation Loss: 2.1190\n",
      "Epoch 916/1000, Train Loss: 14.3742, Validation Loss: 1.9662\n",
      "Epoch 921/1000, Train Loss: 14.4186, Validation Loss: 2.5508\n",
      "Epoch 926/1000, Train Loss: 14.3868, Validation Loss: 2.2441\n",
      "Epoch 931/1000, Train Loss: 14.3590, Validation Loss: 2.0860\n",
      "Epoch 936/1000, Train Loss: 14.3554, Validation Loss: 2.4736\n",
      "Epoch 941/1000, Train Loss: 14.3801, Validation Loss: 2.0662\n",
      "Epoch 946/1000, Train Loss: 14.3623, Validation Loss: 2.1853\n",
      "Epoch 951/1000, Train Loss: 14.3653, Validation Loss: 2.4398\n",
      "Epoch 956/1000, Train Loss: 14.3290, Validation Loss: 2.2008\n",
      "Epoch 961/1000, Train Loss: 14.2991, Validation Loss: 2.1396\n",
      "Epoch 966/1000, Train Loss: 14.3138, Validation Loss: 2.0359\n",
      "Epoch 971/1000, Train Loss: 14.3391, Validation Loss: 2.2349\n",
      "Epoch 976/1000, Train Loss: 14.3471, Validation Loss: 2.1867\n",
      "Epoch 981/1000, Train Loss: 14.3189, Validation Loss: 2.0881\n",
      "Epoch 986/1000, Train Loss: 14.3435, Validation Loss: 1.9711\n",
      "Epoch 991/1000, Train Loss: 14.3385, Validation Loss: 2.0991\n",
      "Epoch 996/1000, Train Loss: 14.4185, Validation Loss: 2.0385\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Initialize variables to track the best model\n",
    "best_val_loss = np.inf  # Set to infinity initially\n",
    "best_checkpoint_path = \"/home/research/a.naveen/denoise40/weather/cnn/best_lstmcnn_model.pth\"\n",
    "\n",
    "train_losses=[]\n",
    "val_losses=[]\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch, city_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch, city_batch)\n",
    "        loss = loss_fn(output.squeeze(), y_batch.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    if epoch%5==0:\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch, city_batch in test_loader:\n",
    "                output = model(X_batch, city_batch)\n",
    "                loss = loss_fn(output.squeeze(), y_batch.squeeze())\n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        # Calculate average losses\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        val_loss /= len(test_loader.dataset)\n",
    "\n",
    "        # Append losses for plotting\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Save the model if validation loss improves\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss\n",
    "            }, best_checkpoint_path)\n",
    "            print(f\"Best model saved at epoch {epoch+1} with validation loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# LSTM Model with Attention and Embedding\n",
    "class LSTMModelWithAttention(nn.Module):\n",
    "    def __init__(self, input_dim, num_cities, embedding_dim, hidden_size, dropout):\n",
    "        super().__init__()\n",
    "        self.city_embedding = nn.Embedding(num_embeddings=num_cities, embedding_dim=embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim + embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Attention Layer\n",
    "        self.attention = nn.Linear(hidden_size * 2, 1)  # Bidirectional LSTM output size is `hidden_size * 2`\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.linear1 = nn.Linear(hidden_size * 2, 64)\n",
    "        self.linear2 = nn.Linear(64, 8)\n",
    "        self.output_linear = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x, city_idx):\n",
    "        # City embedding\n",
    "        city_emb = self.city_embedding(city_idx).unsqueeze(1).expand(-1, x.size(1), -1)\n",
    "        x = torch.cat([x, city_emb], dim=2)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_output, _ = self.lstm(x)  # Shape: (batch_size, seq_len, hidden_size * 2)\n",
    "        \n",
    "        # Attention Mechanism\n",
    "        attention_weights = torch.softmax(self.attention(lstm_output), dim=1)  # Shape: (batch_size, seq_len, 1)\n",
    "        context_vector = torch.sum(attention_weights * lstm_output, dim=1)  # Weighted sum over timesteps\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.dropout(context_vector)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.output_linear(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = X_train.shape[2]  # Number of features\n",
    "num_cities = city_train.max().item() + 1  # Total number of unique cities\n",
    "epochs = 1000\n",
    "hidden_size = 128\n",
    "dropout = 0.39\n",
    "batch_size = 64\n",
    "embedding_dim = 7  # Dimension of the embedding layer\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "model = LSTMModelWithAttention(input_dim=input_dim, num_cities=num_cities, embedding_dim=embedding_dim, hidden_size=hidden_size, dropout=dropout).to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "# Convert data to PyTorch tensors and move to device\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "city_train_tensor = torch.tensor(city_train, dtype=torch.long).to(device)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "city_test_tensor = torch.tensor(city_test, dtype=torch.long).to(device)\n",
    "\n",
    "# DataLoader for batching\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor, city_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor, city_test_tensor)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Train Loss: 52.4353, Validation Loss: 2.3194\n",
      "Best model saved at epoch 1 with validation loss: 2.3194\n",
      "Epoch 6/1000, Train Loss: 2.1198, Validation Loss: 1.8095\n",
      "Best model saved at epoch 6 with validation loss: 1.8095\n",
      "Epoch 11/1000, Train Loss: 1.9548, Validation Loss: 1.7551\n",
      "Best model saved at epoch 11 with validation loss: 1.7551\n",
      "Epoch 16/1000, Train Loss: 1.8474, Validation Loss: 1.5817\n",
      "Best model saved at epoch 16 with validation loss: 1.5817\n",
      "Epoch 21/1000, Train Loss: 1.7777, Validation Loss: 1.5027\n",
      "Best model saved at epoch 21 with validation loss: 1.5027\n",
      "Epoch 26/1000, Train Loss: 1.7433, Validation Loss: 1.5193\n",
      "Epoch 31/1000, Train Loss: 1.7130, Validation Loss: 1.4482\n",
      "Best model saved at epoch 31 with validation loss: 1.4482\n",
      "Epoch 36/1000, Train Loss: 1.6881, Validation Loss: 1.4296\n",
      "Best model saved at epoch 36 with validation loss: 1.4296\n",
      "Epoch 41/1000, Train Loss: 1.6746, Validation Loss: 1.4128\n",
      "Best model saved at epoch 41 with validation loss: 1.4128\n",
      "Epoch 46/1000, Train Loss: 1.6558, Validation Loss: 1.4402\n",
      "Epoch 51/1000, Train Loss: 1.6461, Validation Loss: 1.4181\n",
      "Epoch 56/1000, Train Loss: 1.6340, Validation Loss: 1.4612\n",
      "Epoch 61/1000, Train Loss: 1.6247, Validation Loss: 1.4048\n",
      "Best model saved at epoch 61 with validation loss: 1.4048\n",
      "Epoch 66/1000, Train Loss: 1.6145, Validation Loss: 1.3581\n",
      "Best model saved at epoch 66 with validation loss: 1.3581\n",
      "Epoch 71/1000, Train Loss: 1.6128, Validation Loss: 1.3896\n",
      "Epoch 76/1000, Train Loss: 1.6014, Validation Loss: 1.3992\n",
      "Epoch 81/1000, Train Loss: 1.5887, Validation Loss: 1.3600\n",
      "Epoch 86/1000, Train Loss: 1.5904, Validation Loss: 1.3614\n",
      "Epoch 91/1000, Train Loss: 1.5818, Validation Loss: 1.3699\n",
      "Epoch 96/1000, Train Loss: 1.5756, Validation Loss: 1.3706\n",
      "Epoch 101/1000, Train Loss: 1.5736, Validation Loss: 1.3848\n",
      "Epoch 106/1000, Train Loss: 1.5654, Validation Loss: 1.3615\n",
      "Epoch 111/1000, Train Loss: 1.5635, Validation Loss: 1.3568\n",
      "Best model saved at epoch 111 with validation loss: 1.3568\n",
      "Epoch 116/1000, Train Loss: 1.5563, Validation Loss: 1.3786\n",
      "Epoch 121/1000, Train Loss: 1.5505, Validation Loss: 1.3893\n",
      "Epoch 126/1000, Train Loss: 1.5431, Validation Loss: 1.3522\n",
      "Best model saved at epoch 126 with validation loss: 1.3522\n",
      "Epoch 131/1000, Train Loss: 1.5460, Validation Loss: 1.3649\n",
      "Epoch 136/1000, Train Loss: 1.5434, Validation Loss: 1.3645\n",
      "Epoch 141/1000, Train Loss: 1.5389, Validation Loss: 1.4138\n",
      "Epoch 146/1000, Train Loss: 1.5302, Validation Loss: 1.3781\n",
      "Epoch 151/1000, Train Loss: 1.5272, Validation Loss: 1.3603\n",
      "Epoch 156/1000, Train Loss: 1.5182, Validation Loss: 1.3699\n",
      "Epoch 161/1000, Train Loss: 1.5199, Validation Loss: 1.3729\n",
      "Epoch 166/1000, Train Loss: 1.5113, Validation Loss: 1.3772\n",
      "Epoch 171/1000, Train Loss: 1.5117, Validation Loss: 1.3719\n",
      "Epoch 176/1000, Train Loss: 1.5118, Validation Loss: 1.3730\n",
      "Epoch 181/1000, Train Loss: 1.5019, Validation Loss: 1.3816\n",
      "Epoch 186/1000, Train Loss: 1.5027, Validation Loss: 1.3744\n",
      "Epoch 191/1000, Train Loss: 1.4973, Validation Loss: 1.4111\n",
      "Epoch 196/1000, Train Loss: 1.4947, Validation Loss: 1.3998\n",
      "Epoch 201/1000, Train Loss: 1.4934, Validation Loss: 1.4101\n",
      "Epoch 206/1000, Train Loss: 1.4863, Validation Loss: 1.3952\n",
      "Epoch 211/1000, Train Loss: 1.4787, Validation Loss: 1.3866\n",
      "Epoch 216/1000, Train Loss: 1.4846, Validation Loss: 1.4171\n",
      "Epoch 221/1000, Train Loss: 1.4849, Validation Loss: 1.4114\n",
      "Epoch 226/1000, Train Loss: 1.4772, Validation Loss: 1.3967\n",
      "Epoch 231/1000, Train Loss: 1.4783, Validation Loss: 1.4234\n",
      "Epoch 236/1000, Train Loss: 1.4740, Validation Loss: 1.4086\n",
      "Epoch 241/1000, Train Loss: 1.4687, Validation Loss: 1.4265\n",
      "Epoch 246/1000, Train Loss: 1.4686, Validation Loss: 1.4103\n",
      "Epoch 251/1000, Train Loss: 1.4656, Validation Loss: 1.4045\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     13\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch, city_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     16\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(X_batch, city_batch)\n",
      "File \u001b[0;32m/project/cigserver5/export1/a.naveen/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/project/cigserver5/export1/a.naveen/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:756\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "File \u001b[0;32m/project/cigserver5/export1/a.naveen/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:691\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 691\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter)\n",
      "File \u001b[0;32m/project/cigserver5/export1/a.naveen/venv/lib/python3.12/site-packages/torch/utils/data/sampler.py:350\u001b[0m, in \u001b[0;36mBatchSampler.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    348\u001b[0m batch[idx_in_batch] \u001b[38;5;241m=\u001b[39m idx\n\u001b[1;32m    349\u001b[0m idx_in_batch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx_in_batch \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m batch\n\u001b[1;32m    352\u001b[0m     idx_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Initialize variables to track the best model\n",
    "best_val_loss = np.inf  # Set to infinity initially\n",
    "best_checkpoint_path = \"/home/research/a.naveen/denoise40/weather/lstm/best_attention_model.pth\"\n",
    "\n",
    "train_losses=[]\n",
    "val_losses=[]\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch, city_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch, city_batch)\n",
    "        loss = loss_fn(output.squeeze(), y_batch.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    if epoch%5==0:\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch, city_batch in test_loader:\n",
    "                output = model(X_batch, city_batch)\n",
    "                loss = loss_fn(output.squeeze(), y_batch.squeeze())\n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        # Calculate average losses\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        val_loss /= len(test_loader.dataset)\n",
    "\n",
    "        # Append losses for plotting\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Save the model if validation loss improves\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss\n",
    "            }, best_checkpoint_path)\n",
    "            print(f\"Best model saved at epoch {epoch+1} with validation loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem                                           Size  Used Avail Use% Mounted on\r\n",
      "devtmpfs                                             378G     0  378G   0% /dev\r\n",
      "tmpfs                                                378G  5.8M  378G   1% /dev/shm\r\n",
      "tmpfs                                                378G  4.0G  374G   2% /run\r\n",
      "tmpfs                                                378G     0  378G   0% /sys/fs/cgroup\r\n",
      "/dev/sda2                                             64G   44G   21G  69% /\r\n",
      "/dev/nvme0n1p1                                       3.5T   25G  3.5T   1% /scratch\r\n",
      "/dev/sda1                                            2.0G  437M  1.6G  22% /boot\r\n",
      "/dev/sda5                                            373G  2.7G  370G   1% /tmp\r\n",
      "nfs.seas.wustl.edu:/seaslab/compute                 1000G  606G  395G  61% /project/compute\r\n",
      "nfs.seas.wustl.edu:/seaslab/home-compute              20T   12T  7.6T  62% /home/research\r\n",
      "cigserver3.engr.wustl.edu:/export1/project           3.7T  2.8T  881G  77% /project/cigserver3/export1\r\n",
      "nfs.seas.wustl.edu:/seaslab/home-secure               20T   12T  7.6T  62% /home/home-secure\r\n",
      "cigserver5.engr.wustl.edu:/export1/project            37T   36T  985G  98% /project/cigserver5/export1\r\n",
      "nfs.seas.wustl.edu:/seaslab/engineering             1000G  930G   71G  93% /project/engineering\r\n",
      "nfs.seas.wustl.edu:/seaslab/research                 500G  375G  126G  75% /project/research\r\n",
      "//storage1.ris.wustl.edu/sibai                       5.0T  586G  4.5T  12% /storage1/sibai\r\n",
      "cigserver5.engr.wustl.edu:/export/project            811G  777G   35G  96% /project/cigserver5/export\r\n",
      "cigserver4.engr.wustl.edu:/export3/project            13T   12T  211G  99% /project/cigserver4/export3\r\n",
      "osprey.engr.wustl.edu:/scratch                        21T   21T  296G  99% /project/osprey/scratch\r\n",
      "//storage1.ris.wustl.edu/yvorobeychik                7.0T  5.0T  2.1T  71% /storage1/yvorobeychik\r\n",
      "tantra.engr.wustl.edu:/export1/project/tantra        8.2T  5.6T  2.7T  68% /project/tantra\r\n",
      "//storage1.ris.wustl.edu/chongjie                    5.0T   77G  5.0T   2% /storage1/chongjie\r\n",
      "//storage1.ris.wustl.edu/yinjie.tang                 5.0T  4.7T  323G  94% /storage1/yinjie.tang\r\n",
      "//storage2.ris.wustl.edu/yixin.chen                  5.0T  1.1T  4.0T  21% /storage2/fs1/yixin.chen\r\n",
      "nfs.seas.wustl.edu:/seaslab/home-lab                  20T   12T  7.6T  62% /home/home-lab\r\n",
      "//storage1.ris.wustl.edu/jin.zhang                    45T   41T  4.6T  90% /storage1/jin.zhang\r\n",
      "//storage1.ris.wustl.edu/bsinopoli                   5.0T  113G  4.9T   3% /storage1/bsinopoli\r\n",
      "tmpfs                                                 76G     0   76G   0% /run/user/2006673\r\n",
      "compute-scratch02.engr.wustl.edu:/export/scratch01   1.0T  745G  279G  73% /project/scratch01\r\n",
      "tmpfs                                                 76G     0   76G   0% /run/user/2075617\r\n",
      "tmpfs                                                 76G     0   76G   0% /run/user/2014312\r\n",
      "//storage1.ris.wustl.edu/fuhai.li                     20T   10T   11T  50% /storage1/fuhai.li\r\n",
      "gnode04.engr.wustl.edu:/export/scratch               837G  694G  143G  83% /project/xzgroup-scratch\r\n",
      "//storage1.ris.wustl.edu/christopherking             5.0T  3.8T  1.3T  76% /storage1/christopherking\r\n",
      "gnode04.engr.wustl.edu:/export1/project/xzgroup-gpu  9.1T  8.7T  433G  96% /project/xzgroup-gpu\r\n",
      "ray.engr.wustl.edu:/export/project/xzgroup            15T  5.0T  9.7T  34% /project/xzgroup\r\n",
      "cigserver1.engr.wustl.edu:/export1/project           836G  836G  1.0M 100% /project/cigserver1/export1\r\n",
      "tmpfs                                                 76G     0   76G   0% /run/user/0\r\n"
     ]
    }
   ],
   "source": [
    "!df -h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.0\n",
      "CUDA is available\n",
      "CUDA version: 11.8\n",
      "The history saving thread hit an unexpected error (OperationalError('disk I/O error')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check PyTorch version\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Check CUDA availability and version\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "else:\n",
    "    print(\"CUDA is not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
