{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Morphisms: Mathematical Formulation\n",
    "\n",
    "## 1. Morphism Space Definition\n",
    "Let $\\mathcal{M} = \\{S, X, Y\\}$ be the morphism space, where:\n",
    "\n",
    "- $S$: Sample Space\n",
    "- $X$: Feature Space\n",
    "- $Y$: Target Space\n",
    "\n",
    "### 1.1 Morphism Transformation Operator\n",
    "Define the morphism transformation operator $\\mathcal{M}: S \\to Y$ as:\n",
    "\n",
    "$$\\mathcal{M}(s) = \\bigcup_{i=1}^{k} F_i(s)$$\n",
    "\n",
    "Where $F_i$ represents different transformation functions.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Data Preprocessing Morphism $\\mathcal{M}_1$\n",
    "\n",
    "### 2.1 Preprocessing Transformation\n",
    "$$\\mathcal{M}_1: S \\to X$$\n",
    "\n",
    "$$X = \\{\\xi \\mid \\xi = f_{prep}(s), s \\in S\\}$$\n",
    "\n",
    "Preprocessing functions:\n",
    "\n",
    "- Null value handling: $f_{null}(s)$\n",
    "- Outlier detection: $f_{outlier}(s)$\n",
    "  - IQR-based outlier removal\n",
    "  - Mahalanobis distance-based outlier detection\n",
    "  - Minimum Covariance Determinant (MCD) outlier detection\n",
    "  - Time series decomposition-based outlier removal\n",
    "- Standardization: $f_{std}(s)$\n",
    "\n",
    "The preprocessing pipeline:\n",
    "\n",
    "$$f_{prep}(s) = f_{std}(f_{outlier}(f_{null}(s)))$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Outlier Detection Methods\n",
    "\n",
    "#### 2.2.1 IQR-based Outlier Removal\n",
    "Outliers are identified using the Interquartile Range (IQR):\n",
    "\n",
    "1. Compute the first ($Q_1$) and third quartiles ($Q_3$) of the data.\n",
    "2. Calculate the IQR:\n",
    "   $$\n",
    "   IQR = Q_3 - Q_1\n",
    "   $$\n",
    "3. Define lower and upper bounds:\n",
    "   $$\n",
    "   \\text{Lower Bound} = Q_1 - 1.5 \\cdot IQR\n",
    "   $$\n",
    "   $$\n",
    "   \\text{Upper Bound} = Q_3 + 1.5 \\cdot IQR\n",
    "   $$\n",
    "4. Identify outliers:\n",
    "   $$\n",
    "   x \\text{ is an outlier if } x < \\text{Lower Bound or } x > \\text{Upper Bound.}\n",
    "   $$\n",
    "\n",
    "\n",
    "\n",
    "#### 2.2.2 Mahalanobis Distance-based Outlier Detection\n",
    "Mahalanobis distance is used to measure the distance of a point from the center of a multivariate distribution:\n",
    "\n",
    "1. Compute the mean vector $\\mu$ and covariance matrix $\\Sigma$ of the dataset.\n",
    "2. Calculate the Mahalanobis distance for each data point $x$:\n",
    "   $$\n",
    "   D_M(x) = \\sqrt{(x - \\mu)^T \\Sigma^{-1} (x - \\mu)}\n",
    "   $$\n",
    "3. Define a threshold (e.g., based on the Chi-squared distribution) to classify outliers:\n",
    "   $$\n",
    "   D_M(x) > \\text{Threshold} \\implies x \\text{ is an outlier.}\n",
    "   $$\n",
    "\n",
    "\n",
    "\n",
    "#### 2.2.3 Minimum Covariance Determinant (MCD) Outlier Detection\n",
    "The MCD method is a robust estimator of covariance and location:\n",
    "\n",
    "1. Compute the MCD location $\\mu_{MCD}$ and covariance $\\Sigma_{MCD}$ from the data.\n",
    "2. Calculate the Mahalanobis distance using MCD estimators:\n",
    "   $$\n",
    "   D_{MCD}(x) = \\sqrt{(x - \\mu_{MCD})^T \\Sigma_{MCD}^{-1} (x - \\mu_{MCD})}\n",
    "   $$\n",
    "3. Identify outliers:\n",
    "   $$\n",
    "   D_{MCD}(x) > \\text{Threshold} \\implies x \\text{ is an outlier.}\n",
    "   $$\n",
    "\n",
    "\n",
    "\n",
    "#### 2.2.4 Time Series Decomposition-based Outlier Removal\n",
    "Outliers in time series data are identified by decomposing the data into trend, seasonality, and residuals:\n",
    "\n",
    "1. Decompose the time series:\n",
    "   $$\n",
    "   x_t = T_t + S_t + R_t\n",
    "   $$\n",
    "   Where:\n",
    "   - $T_t$: Trend component\n",
    "   - $S_t$: Seasonal component\n",
    "   - $R_t$: Residual (random) component\n",
    "2. Identify outliers in the residual component:\n",
    "   $$\n",
    "   R_t \\text{ is an outlier if } |R_t| > k \\cdot \\sigma(R)\n",
    "   $$\n",
    "   Where $\\sigma(R)$ is the standard deviation of the residuals and $k$ is a chosen threshold (e.g., $k=3$ for 3 standard deviations).\n",
    "\n",
    "\n",
    "\n",
    "### 2.3 Standardization Function\n",
    "$$f_{std}(x) = \\frac{x - \\mu(X)}{\\sigma(X)}$$\n",
    "\n",
    "Where:\n",
    "- $\\mu(X)$ is the mean of the feature $X$.\n",
    "- $\\sigma(X)$ is the standard deviation of the feature $X$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Feature Engineering Morphism $\\mathcal{M}_2$\n",
    "\n",
    "### 3.1 Feature Transformation\n",
    "$$\\mathcal{M}_2: X \\to X'$$\n",
    "\n",
    "$$X' = \\{\\xi' \\mid \\xi' = f_{eng}(\\xi), \\xi \\in X\\}$$\n",
    "\n",
    "Feature engineering operations:\n",
    "\n",
    "- **Log transformation**: $f_{log}(x)$\n",
    "- **Correlation analysis**: $f_{corr}(X)$\n",
    "- **Feature selection**: $f_{select}(X)$\n",
    "- **One-hot encoding for categorical variables**: $f_{onehot}(X)$\n",
    "\n",
    "The feature engineering pipeline:\n",
    "\n",
    "$$f_{eng}(x) = f_{select}(f_{corr}(f_{log}(f_{onehot}(x))))$$\n",
    "\n",
    "\n",
    "\n",
    "### 3.2 Correlation Matrix Computation\n",
    "The correlation matrix $\\rho$ measures the pairwise relationships between features:\n",
    "\n",
    "$$\\rho_{i,j} = \\frac{Cov(X_i, X_j)}{\\sigma(X_i)\\sigma(X_j)}$$\n",
    "\n",
    "Where:\n",
    "- $Cov(X_i, X_j)$ is the covariance between features $X_i$ and $X_j$.\n",
    "- $\\sigma(X_i)$ and $\\sigma(X_j)$ are the standard deviations of $X_i$ and $X_j$, respectively.\n",
    "\n",
    "\n",
    "\n",
    "### 3.3 Feature Selection Based on High Correlation\n",
    "Highly correlated independent features introduce redundancy. Features with a correlation coefficient $\\rho > \\tau$ (threshold) are removed:\n",
    "\n",
    "1. Compute the correlation matrix $\\rho$ for all independent features.\n",
    "2. Define a threshold $\\tau$ (e.g., $\\tau = 0.85$).\n",
    "3. For each pair $(X_i, X_j)$ where $\\rho_{i,j} > \\tau$:\n",
    "   - Remove one of the features $X_i$ or $X_j$, typically based on domain knowledge or variance contribution.\n",
    "\n",
    "Mathematically:\n",
    "$$\n",
    "X' = \\{X_i \\in X \\mid \\forall j, \\rho_{i,j} \\leq \\tau \\text{ or } i = j\\}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### 3.4 One-Hot Encoding for Categorical Variables\n",
    "Categorical variables are transformed into numerical representations using one-hot encoding:\n",
    "\n",
    "1. For a categorical variable $C$ with $k$ unique categories $\\{c_1, c_2, \\dots, c_k\\}$:\n",
    "   - Create $k$ binary features $\\{C_1, C_2, \\dots, C_k\\}$, where:\n",
    "     $$\n",
    "     C_j = \n",
    "     \\begin{cases} \n",
    "     1 & \\text{if } C = c_j \\\\\n",
    "     0 & \\text{otherwise}\n",
    "     \\end{cases}\n",
    "     $$\n",
    "2. Replace $C$ in the dataset with the binary features $\\{C_1, C_2, \\dots, C_k\\}$.\n",
    "\n",
    "Let $X_{cat}$ represent the set of categorical features in $X$. The transformed dataset after one-hot encoding is:\n",
    "$$\n",
    "f_{onehot}(X) = (X \\setminus X_{cat}) \\cup \\bigcup_{C \\in X_{cat}} \\{C_1, C_2, \\dots, C_k\\}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Final Feature Engineering Pipeline\n",
    "The complete feature engineering process:\n",
    "1. Apply one-hot encoding for categorical variables:\n",
    "   $$\n",
    "   X \\to f_{onehot}(X)\n",
    "   $$\n",
    "2. Apply log transformation to numerical features to reduce skewness:\n",
    "   $$\n",
    "   X \\to f_{log}(X)\n",
    "   $$\n",
    "3. Perform correlation analysis and remove features with $\\rho > \\tau$:\n",
    "   $$\n",
    "   X \\to f_{corr}(X)\n",
    "   $$\n",
    "4. Retain the most relevant features for modeling:\n",
    "   $$\n",
    "   X \\to f_{select}(X)\n",
    "   $$\n",
    "\n",
    "The resulting feature set is:\n",
    "$$\n",
    "X' = f_{eng}(X)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Time Series Stationarity Morphism $\\mathcal{M}_3$\n",
    "\n",
    "### 4.1 Stationarity Transformation\n",
    "$$\\mathcal{M}_3: X' \\to X''$$\n",
    "\n",
    "$$X'' = \\{\\xi'' \\mid \\xi'' = f_{stat}(\\xi'), \\xi' \\in X'\\}$$\n",
    "\n",
    "Stationarity tests, transformations, and diagnostics:\n",
    "\n",
    "- **Augmented Dickey-Fuller (ADF) test**: $f_{adf}(x)$\n",
    "- **Differencing**: $f_{diff}(x)$\n",
    "- **Seasonal decomposition**: $f_{seasonal}(x)$\n",
    "- **Autocorrelation Function (ACF) plot**: $f_{acf}(x)$\n",
    "- **Partial Autocorrelation Function (PACF) plot**: $f_{pacf}(x)$\n",
    "\n",
    "$$\n",
    "f_{stat}(x') = \n",
    "\\begin{cases} \n",
    "x' & \\text{if } f_{adf}(x') \\text{ is stationary} \\\\\n",
    "f_{diff}(x') & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### 4.2 ACF and PACF Plot Computation\n",
    "To assess autocorrelation and partial autocorrelation, the following plots are generated:\n",
    "\n",
    "- **ACF Plot**:\n",
    "  $$ACF(lag) = \\frac{\\sum_{t=1}^{T-lag} (x_t - \\mu)(x_{t+lag} - \\mu)}{\\sum_{t=1}^{T} (x_t - \\mu)^2}$$\n",
    "\n",
    "- **PACF Plot**:\n",
    "  The PACF at lag $k$ is calculated by fitting autoregressive models of order $k$ and examining the correlation between $x_t$ and $x_{t-k}$ after removing the effects of the intermediate lags $1, 2, \\dots, k-1$.\n",
    "\n",
    "These plots are used to:\n",
    "1. Identify the order of differencing needed (if any).\n",
    "2. Determine the parameters $p$ (AR order) and $q$ (MA order) for SARIMA modeling.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## 5. Model Training Morphism $\\mathcal{M}_4$\n",
    "\n",
    "### 5.1 Model Representation\n",
    "$$\n",
    "\\mathcal{M}_4: X'' \\to \\mathcal{M}, \\quad M \\in \\mathcal{M}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Where $M$ is the set of models:\n",
    "\n",
    "1. **Baseline Model**: $M_{SARIMA}$\n",
    "2. **1D-CNN**: $M_{1D-CNN}$\n",
    "3. **LSTM**: $M_{LSTM}$\n",
    "4. **LSTM with Attention**: $M_{LSTM-Attention}$\n",
    "\n",
    "---\n",
    "\n",
    "### 5.1.1 Baseline Model: SARIMAX ($M_{SARIMAX}$)\n",
    "\n",
    "The **Seasonal Autoregressive Integrated Moving Average with Exogenous Regressors (SARIMAX)** model includes both time series components and exogenous features $Z_t$ (e.g., wind speed, humidity, cloud cover). The model is represented as:\n",
    "\n",
    "$$\n",
    "\\Phi_p(B)(1 - B)^d (1 - B^s)^D y_t = \\Theta_q(B) \\Theta_Q(B^s) \\varepsilon_t + \\beta Z_t\n",
    "$$\n",
    "\n",
    "- **Components**:\n",
    "  - $B$: Backshift operator, where $B y_t = y_{t-1}$.\n",
    "  - $\\Phi_p(B)$: Non-seasonal AR polynomial of order $p$:\n",
    "    $$\n",
    "    \\Phi_p(B) = 1 - \\phi_1 B - \\phi_2 B^2 - \\cdots - \\phi_p B^p\n",
    "    $$\n",
    "  - $\\Theta_q(B)$: Non-seasonal MA polynomial of order $q$:\n",
    "    $$\n",
    "    \\Theta_q(B) = 1 + \\theta_1 B + \\theta_2 B^2 + \\cdots + \\theta_q B^q\n",
    "    $$\n",
    "  - $d$: Non-seasonal differencing order.\n",
    "  - $D$: Seasonal differencing order.\n",
    "  - $s$: Seasonality period (e.g., 12 for monthly data).\n",
    "  - $Z_t$: Exogenous predictors at time $t$ (e.g., weather variables such as temperature, wind speed, etc.).\n",
    "  - $\\beta$: Coefficient vector for the exogenous predictors.\n",
    "  - $\\varepsilon_t$: White noise at time $t$.\n",
    "\n",
    "#### **Objective for SARIMAX**:\n",
    "SARIMAX minimizes the residual sum of squares with respect to both the ARIMA parameters $(p, d, q, P, D, Q, s)$ and the exogenous coefficients $\\beta$:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{SARIMAX} = \\sum_{t=1}^n (y_t - \\hat{y}_t)^2\n",
    "$$\n",
    "\n",
    "#### **Diagnostics and Parameter Selection**:\n",
    "1. Use **ACF** and **PACF plots** for $p$ and $q$ selection.\n",
    "2. Use feature importance analysis for selecting relevant exogenous predictors $Z_t$.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.1.2 1D-CNN ($M_{1D-CNN}$)\n",
    "\n",
    "The **1D-CNN** model for weather prediction captures local temporal patterns in the data. Its forward pass is defined as:\n",
    "\n",
    "$$\n",
    "h^{(l)} = f(W^{(l)} * X^{(l-1)} + b^{(l)})\n",
    "$$\n",
    "\n",
    "- **Components**:\n",
    "  - $X^{(l-1)}$: Input tensor from the previous layer.\n",
    "  - $W^{(l)}$: Convolutional kernel (filter).\n",
    "  - $*$: Convolution operator.\n",
    "  - $b^{(l)}$: Bias term for layer $l$.\n",
    "  - $f$: Activation function (e.g., ReLU or tanh).\n",
    "  - $h^{(l)}$: Output tensor of layer $l$.\n",
    "\n",
    "The CNN is trained to minimize the Mean Squared Error (MSE):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{1D-CNN} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 5.1.3 LSTM ($M_{LSTM}$)\n",
    "\n",
    "The **Long Short-Term Memory (LSTM)** model is used to capture long-term dependencies in the weather data. Its equations are:\n",
    "\n",
    "1. Forget gate:\n",
    "   $$\n",
    "   f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "   $$\n",
    "2. Input gate:\n",
    "   $$\n",
    "   i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
    "   $$\n",
    "   $$\n",
    "   \\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
    "   $$\n",
    "3. Cell state update:\n",
    "   $$\n",
    "   C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t\n",
    "   $$\n",
    "4. Output gate:\n",
    "   $$\n",
    "   o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "   $$\n",
    "   $$\n",
    "   h_t = o_t \\odot \\tanh(C_t)\n",
    "   $$\n",
    "\n",
    "The LSTM model is trained by minimizing the Mean Absolute Error (MAE):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{LSTM} = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 5.1.4 LSTM with Attention ($M_{LSTM-Attention}$)\n",
    "\n",
    "The **LSTM with Attention** model improves predictions by focusing on the most relevant time steps in the sequence. The attention mechanism is defined as:\n",
    "\n",
    "1. Compute attention scores:\n",
    "   $$\n",
    "   e_t = \\text{score}(h_t, s)\n",
    "   $$\n",
    "   where $\\text{score}$ is a function (e.g., dot product or alignment).\n",
    "\n",
    "2. Normalize scores using softmax:\n",
    "   $$\n",
    "   \\alpha_t = \\frac{\\exp(e_t)}{\\sum_{t'} \\exp(e_{t'})}\n",
    "   $$\n",
    "\n",
    "3. Compute context vector:\n",
    "   $$\n",
    "   c = \\sum_t \\alpha_t h_t\n",
    "   $$\n",
    "\n",
    "4. Combine context vector and LSTM output:\n",
    "   $$\n",
    "   \\hat{y}_t = f(c, h_t)\n",
    "   $$\n",
    "\n",
    "The attention-enhanced predictions are evaluated using Mean Squared Error (MSE):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{LSTM-Attention} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 5.2 Loss Function\n",
    "The general loss function for all models is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(M) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Each model adapts this loss function to its architecture and feature representation.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Hyperparameter Tuning Morphism $\\mathcal{M}_6$\n",
    "\n",
    "### 6.1 Tuning with Optuna\n",
    "Optuna is a hyperparameter optimization framework that intelligently searches the hyperparameter space $\\Theta$ to minimize the objective function $\\mathcal{L}(M)$:\n",
    "\n",
    "$$\n",
    "\\theta^* = \\underset{\\theta \\in \\Theta}{\\arg\\min} \\mathcal{L}(M(\\theta))\n",
    "$$\n",
    "\n",
    "Optuna works internally using the following steps:\n",
    "\n",
    "---\n",
    "\n",
    "#### 6.1.1 Trial and Objective Function Morphism\n",
    "Each **trial** represents a single evaluation of the objective function $\\mathcal{L}(M)$ for a given set of hyperparameters $\\theta$. Let the trial space $\\mathcal{T}$ be:\n",
    "\n",
    "$$\n",
    "\\mathcal{T} = \\{\\tau_1, \\tau_2, \\dots, \\tau_k\\}\n",
    "$$\n",
    "\n",
    "For each trial $\\tau_i$, Optuna performs:\n",
    "1. **Sampling**: Generate a candidate hyperparameter set $\\theta_i$ from the search space $\\Theta$:\n",
    "   $$\n",
    "   \\theta_i \\sim P(\\Theta)\n",
    "   $$\n",
    "   Where $P(\\Theta)$ is the prior distribution over $\\Theta$.\n",
    "2. **Evaluation**: Evaluate the model $M$ using the sampled hyperparameters:\n",
    "   $$\n",
    "   \\mathcal{L}(M(\\theta_i)) = \\frac{1}{n}\\sum_{j=1}^n (y_j - \\hat{y}_j)^2\n",
    "   $$\n",
    "3. **Storage**: Store the results of the trial $(\\theta_i, \\mathcal{L}(M(\\theta_i)))$ in the study history.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6.1.2 Search Strategy Morphism\n",
    "Optuna dynamically selects hyperparameters based on the results of previous trials using one of the following strategies:\n",
    "\n",
    "1. **Random Search**:\n",
    "   $$\n",
    "   \\theta \\sim \\text{Uniform}(\\Theta)\n",
    "   $$\n",
    "   This explores the search space randomly, with no reliance on previous results.\n",
    "\n",
    "2. **Bayesian Optimization**:\n",
    "   Optuna uses a **Tree-structured Parzen Estimator (TPE)** to approximate the objective function. The TPE separates the search space into two likelihood models:\n",
    "   - $P(\\mathcal{L} \\mid \\theta)$: Likelihood of good hyperparameters based on previous trials.\n",
    "   - $P(\\theta)$: Prior distribution of hyperparameters.\n",
    "\n",
    "   The next hyperparameter $\\theta_{next}$ is selected to maximize the expected improvement (EI):\n",
    "   $$\n",
    "   \\theta_{next} = \\underset{\\theta}{\\arg\\max} \\ \\text{EI}(\\theta)\n",
    "   $$\n",
    "   Where:\n",
    "   $$\n",
    "   \\text{EI}(\\theta) = \\int_{-\\infty}^{\\mathcal{L}^*} (\\mathcal{L}^* - \\mathcal{L}) \\cdot P(\\mathcal{L} \\mid \\theta) \\, d\\mathcal{L}\n",
    "   $$\n",
    "   and $\\mathcal{L}^*$ is the best observed objective value.\n",
    "\n",
    "3. **Grid Search**: Optuna can simulate grid search by exhaustively sampling all combinations within a discretized $\\Theta$.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6.1.3 Pruning Morphism\n",
    "Optuna implements pruning to terminate underperforming trials early. Let $\\mathcal{T}_{active} \\subseteq \\mathcal{T}$ represent active trials. For each trial $\\tau_i$:\n",
    "1. Evaluate the intermediate objective value at step $k$:\n",
    "   $$\n",
    "   \\mathcal{L}_k(\\tau_i) = \\frac{1}{k} \\sum_{j=1}^k (y_j - \\hat{y}_j)^2\n",
    "   $$\n",
    "2. Compare with the best-performing trial $\\tau^*$:\n",
    "   $$\n",
    "   \\tau_i \\text{ is pruned if } \\mathcal{L}_k(\\tau_i) > \\mathcal{L}_k(\\tau^*).\n",
    "   $$\n",
    "3. Remove pruned trials from $\\mathcal{T}_{active}$:\n",
    "   $$\n",
    "   \\mathcal{T}_{active} = \\mathcal{T}_{active} \\setminus \\{\\tau_i \\mid \\text{pruned}\\}.\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "#### 6.1.4 Final Optimization Morphism\n",
    "After $k$ trials, Optuna selects the best hyperparameters $\\theta^*$ based on the study history:\n",
    "\n",
    "$$\n",
    "\\theta^* = \\underset{\\theta \\in \\mathcal{T}}{\\arg\\min} \\mathcal{L}(M(\\theta))\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 6.2 Summary of Optuna's Internal Process\n",
    "1. **Trial Generation**:\n",
    "   - Random or guided sampling (e.g., TPE) is used to select $\\theta$.\n",
    "2. **Evaluation**:\n",
    "   - The objective function $\\mathcal{L}(M(\\theta))$ is minimized.\n",
    "3. **Pruning**:\n",
    "   - Poor trials are terminated early based on intermediate performance.\n",
    "4. **Selection**:\n",
    "   - The best hyperparameters $\\theta^*$ are chosen after $k$ trials.\n",
    "\n",
    "By leveraging this dynamic and adaptive process, Optuna efficiently identifies optimal hyperparameters for the model $M$.\n",
    "\n",
    "\n",
    "### 6.2 Time Series Cross-Validation\n",
    "To evaluate model performance during tuning, **time series cross-validation** is applied using Scikit-learn's `TimeSeriesSplit`. This ensures that temporal dependencies are respected.\n",
    "\n",
    "1. **Splitting Mechanism**:\n",
    "   Let $T$ be the total time steps, $t$ be the initial training window size, $v$ be the validation window size, and $k$ be the number of splits.\n",
    "\n",
    "   For each split $i$:\n",
    "   - Training set:\n",
    "     $$\n",
    "     T_{\\text{train}}^{(i)} = \\{1, 2, \\dots, t + (i-1)v\\}\n",
    "     $$\n",
    "   - Validation set:\n",
    "     $$\n",
    "     T_{\\text{val}}^{(i)} = \\{t + (i-1)v + 1, \\dots, t + iv\\}\n",
    "     $$\n",
    "\n",
    "2. **Example**:\n",
    "   If $T = 100$, $t = 60$, $v = 10$, and $k = 4$, the splits are:\n",
    "   - Split 1:\n",
    "     $T_{\\text{train}}^{(1)} = \\{1, 2, \\dots, 60\\}$, $T_{\\text{val}}^{(1)} = \\{61, \\dots, 70\\}$\n",
    "   - Split 2:\n",
    "     $T_{\\text{train}}^{(2)} = \\{1, 2, \\dots, 70\\}$, $T_{\\text{val}}^{(2)} = \\{71, \\dots, 80\\}$\n",
    "   - Split 3:\n",
    "     $T_{\\text{train}}^{(3)} = \\{1, 2, \\dots, 80\\}$, $T_{\\text{val}}^{(3)} = \\{81, \\dots, 90\\}$\n",
    "   - Split 4:\n",
    "     $T_{\\text{train}}^{(4)} = \\{1, 2, \\dots, 90\\}$, $T_{\\text{val}}^{(4)} = \\{91, \\dots, 100\\}$\n",
    "\n",
    "3. **Loss Calculation**:\n",
    "   The cross-validation loss is computed as the average over all splits:\n",
    "   $$\n",
    "   \\mathcal{L}_{CV} = \\frac{1}{k} \\sum_{i=1}^{k} \\mathcal{L}^{(i)}\n",
    "   $$\n",
    "   where $\\mathcal{L}^{(i)}$ is the loss for the $i$-th split.\n",
    "\n",
    "---\n",
    "\n",
    "This morphism describes how hyperparameter tuning and evaluation are performed efficiently using Optuna and time series cross-validation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 7. Model Evaluation Morphism $\\mathcal{M}_7$\n",
    "\n",
    "### 7.1 Performance Metrics\n",
    "\n",
    "**Mean Squared Error (MSE):**\n",
    "\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "**Root Mean Squared Error (RMSE):**\n",
    "\n",
    "$$RMSE = \\sqrt{MSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$\n",
    "\n",
    "**Mean Absolute Error (MAE):**\n",
    "\n",
    "$$MAE = \\frac{1}{n}\\sum_{i=1}^{n} |y_i - \\hat{y}_i|$$\n",
    "\n",
    "**Mean Absolute Percentage Error (MAPE):**\n",
    "\n",
    "$$MAPE = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right| \\times 100$$\n",
    "\n",
    "Where:\n",
    "- $y_i$: Actual value at observation $i$.\n",
    "- $\\hat{y}_i$: Predicted value at observation $i$.\n",
    "- $n$: Number of observations.\n",
    "\n",
    "---\n",
    "\n",
    "### 7.2 Model Selection Criteria\n",
    "\n",
    "**Akaike Information Criterion (AIC):**\n",
    "\n",
    "The AIC is used to evaluate model fit while penalizing model complexity:\n",
    "\n",
    "$$AIC = 2k - 2 \\ln(L)$$\n",
    "\n",
    "Where:\n",
    "- $k$: Number of model parameters.\n",
    "- $L$: Maximum likelihood of the model.\n",
    "\n",
    "**Bayesian Information Criterion (BIC):**\n",
    "\n",
    "The BIC is another metric for model selection, with a stronger penalty for model complexity compared to AIC:\n",
    "\n",
    "$$BIC = k \\ln(n) - 2 \\ln(L)$$\n",
    "\n",
    "Where:\n",
    "- $k$: Number of model parameters.\n",
    "- $n$: Number of data points.\n",
    "- $L$: Maximum likelihood of the model.\n",
    "\n",
    "Both AIC and BIC are minimized to select the best model. Lower values indicate a better balance of model fit and complexity.\n",
    "\n",
    "---\n",
    "\n",
    "### 7.3 Bias-Variance Decomposition\n",
    "\n",
    "The error of a predictive model can be decomposed into three components:\n",
    "\n",
    "$$\\text{Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}$$\n",
    "\n",
    "Where:\n",
    "- **Bias** measures the error due to overly simplistic assumptions in the model.\n",
    "- **Variance** measures the sensitivity of the model to fluctuations in the training data.\n",
    "- **Irreducible Error** represents the noise inherent in the data that no model can explain.\n",
    "\n",
    "---\n",
    "\n",
    "### Final Performance Evaluation\n",
    "A comprehensive evaluation of the models includes:\n",
    "1. **Error Metrics**:\n",
    "   - $MSE$, $RMSE$, $MAE$, $MAPE$\n",
    "2. **Model Selection Criteria**:\n",
    "   - $AIC$, $BIC$\n",
    "3. **Error Decomposition**:\n",
    "   - Bias-Variance tradeoff analysis.\n",
    "\n",
    "## 8. Comprehensive Morphism Composition\n",
    "\n",
    "### 8.1 Total Morphism Operator\n",
    "$$\\mathcal{M}_{total}: S \\to Y$$\n",
    "\n",
    "$$\\mathcal{M}_{total} = \\mathcal{M}_7 \\circ \\mathcal{M}_6 \\circ \\mathcal{M}_5 \\circ \\mathcal{M}_4 \\circ \\mathcal{M}_3 \\circ \\mathcal{M}_2 \\circ \\mathcal{M}_1$$\n",
    "\n",
    "### 8.2 Probabilistic Model Representation\n",
    "$$P(Y|X) = \\int P(Y|M,X)P(M|X)dM$$\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
